{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72531fff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq\n"
     ]
    }
   ],
   "source": [
    "cd ../fairseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train --task language_modeling \\\n",
    "  data-bin/wikitext-103 \\\n",
    "  --save-dir checkpoints/transformer_wikitext-103 \\\n",
    "  --arch transformer_lm --share-decoder-input-output-embed \\\n",
    "  --dropout 0.1 \\\n",
    "  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\n",
    "  --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "  --tokens-per-sample 512 --sample-break-mode none \\\n",
    "  --max-tokens 2048 --update-freq 16 \\\n",
    "  --fp16 \\\n",
    "  --max-update 50000 \\\n",
    "  --wandb-project  NLP_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36dbd872",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 09:16:42 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-01-08 09:16:44 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'NLP_test', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'legacy_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 3072, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3072, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 286000, 'stop_time_hours': 0.0, 'clip_norm': 0.1, 'sentence_avg': False, 'update_freq': [3], 'lr': [1.0], 'stop_min_lr': 1e-09, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/transformer_wikitext-103-AdT', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm_wiki103', 'activation_fn': relu, 'dropout': 0.3, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'relu_dropout': 0.0, 'decoder_embed_dim': 1024, 'decoder_output_dim': 1024, 'decoder_input_dim': 1024, 'decoder_ffn_embed_dim': 4096, 'decoder_layers': 16, 'decoder_attention_heads': 8, 'decoder_normalize_before': True, 'no_decoder_final_norm': True, 'adaptive_softmax_cutoff': '20000,60000', 'adaptive_softmax_dropout': 0.2, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': False, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': True, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': '20000,60000', 'tie_adaptive_weights': True, 'tie_adaptive_proj': True, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 0, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'add_bos_token': False, 'tokens_per_sample': 3072, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103', 'sample_break_mode': none, 'tokens_per_sample': 3072, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'adaptive_loss', 'sentence_avg': False, 'ddp_backend': legacy_ddp}, 'optimizer': {'_name': 'nag', 'momentum': 0.99, 'weight_decay': 0.0, 'lr': [1.0]}, 'lr_scheduler': {'_name': 'cosine', 'warmup_updates': 16000, 'warmup_init_lr': 1e-07, 'lr': [1.0], 'min_lr': 0.0001, 't_mult': 2.0, 'lr_period_updates': 270000.0, 'lr_shrink': 0.75, 'max_update': 286000}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 09:16:44 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types\n",
      "2023-01-08 09:16:45 | INFO | fairseq_cli.train | TransformerLanguageModel(\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): AdaptiveInput(\n",
      "      (embeddings): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): Embedding(20000, 1024, padding_idx=1)\n",
      "          (1): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): Embedding(40000, 256)\n",
      "          (1): Linear(in_features=256, out_features=1024, bias=False)\n",
      "        )\n",
      "        (2): Sequential(\n",
      "          (0): Embedding(207744, 64)\n",
      "          (1): Linear(in_features=64, out_features=1024, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (6): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (7): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (8): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (9): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (10): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (11): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (12): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (13): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (14): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (15): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (adaptive_softmax): AdaptiveSoftmax(\n",
      "      (dropout_module): FairseqDropout()\n",
      "      (lsm): LogSoftmax(dim=1)\n",
      "      (head): TiedHeadModule(\n",
      "        (word_proj): TiedLinear()\n",
      "        (class_proj): Linear(in_features=1024, out_features=2, bias=False)\n",
      "      )\n",
      "      (tail): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (0): TiedLinear()\n",
      "          (1): Dropout(p=0.2, inplace=False)\n",
      "          (2): TiedLinear()\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): TiedLinear()\n",
      "          (1): Dropout(p=0.2, inplace=False)\n",
      "          (2): TiedLinear()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-01-08 09:16:45 | INFO | fairseq_cli.train | task: LanguageModelingTask\n",
      "2023-01-08 09:16:45 | INFO | fairseq_cli.train | model: TransformerLanguageModel\n",
      "2023-01-08 09:16:45 | INFO | fairseq_cli.train | criterion: AdaptiveLoss\n",
      "2023-01-08 09:16:45 | INFO | fairseq_cli.train | num. shared model params: 246,933,504 (num. trained: 246,933,504)\n",
      "2023-01-08 09:16:45 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-01-08 09:16:45 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103/valid\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.0.weight <- decoder.adaptive_softmax.head.word_proj.weight\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.1.1.bias\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.embed_tokens.embeddings.2.1.bias\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.0.1.bias <- decoder.adaptive_softmax.head.class_proj.bias\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.0.weight <- decoder.adaptive_softmax.tail.0.2.weight\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.1.1.weight <- decoder.adaptive_softmax.tail.0.0.weight\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.0.weight <- decoder.adaptive_softmax.tail.1.2.weight\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.embeddings.2.1.weight <- decoder.adaptive_softmax.tail.1.0.weight\n",
      "2023-01-08 09:16:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 09:16:46 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 \n",
      "2023-01-08 09:16:46 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 09:16:46 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-01-08 09:16:46 | INFO | fairseq_cli.train | max tokens per device = 3072 and max sentences per device = None\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/transformer_wikitext-103-AdT/checkpoint_last.pt\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/transformer_wikitext-103-AdT/checkpoint_last.pt\n",
      "2023-01-08 09:16:46 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-01-08 09:16:46 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103/train\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 09:16:47 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
      "2023-01-08 09:16:47 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 09:16:47 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 09:16:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 11201\n",
      "epoch 001:   0%|                                      | 0/11201 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzh110\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/wandb/run-20230108_091649-3p2cay1w\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtransformer_wikitext-103-AdT\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test/runs/3p2cay1w\u001b[0m\n",
      "2023-01-08 09:16:57 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-01-08 09:16:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
      "  warnings.warn(\n",
      "epoch 001: 100%|▉| 11200/11201 [4:04:30<00:01,  1.31s/it, loss=7.708, ppl=209.052023-01-08 13:21:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 13:21:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|               | 0/70 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   1%|       | 1/70 [00:00<00:11,  6.26it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   3%|▏      | 2/70 [00:00<00:10,  6.52it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   4%|▎      | 3/70 [00:00<00:10,  6.66it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   6%|▍      | 4/70 [00:00<00:09,  6.73it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   7%|▌      | 5/70 [00:00<00:09,  6.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   9%|▌      | 6/70 [00:00<00:09,  6.76it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  10%|▋      | 7/70 [00:01<00:09,  6.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  11%|▊      | 8/70 [00:01<00:09,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  13%|▉      | 9/70 [00:01<00:09,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  14%|▊     | 10/70 [00:01<00:08,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  16%|▉     | 11/70 [00:01<00:08,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  17%|█     | 12/70 [00:01<00:08,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  19%|█     | 13/70 [00:01<00:08,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  20%|█▏    | 14/70 [00:02<00:08,  6.76it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  21%|█▎    | 15/70 [00:02<00:08,  6.75it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  23%|█▎    | 16/70 [00:02<00:07,  6.76it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  24%|█▍    | 17/70 [00:02<00:07,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  26%|█▌    | 18/70 [00:02<00:07,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  27%|█▋    | 19/70 [00:02<00:07,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  29%|█▋    | 20/70 [00:02<00:07,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  30%|█▊    | 21/70 [00:03<00:07,  6.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  31%|█▉    | 22/70 [00:03<00:07,  6.76it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  33%|█▉    | 23/70 [00:03<00:06,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  34%|██    | 24/70 [00:03<00:06,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  36%|██▏   | 25/70 [00:03<00:06,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  37%|██▏   | 26/70 [00:03<00:06,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  39%|██▎   | 27/70 [00:03<00:06,  6.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  40%|██▍   | 28/70 [00:04<00:06,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  41%|██▍   | 29/70 [00:04<00:06,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  43%|██▌   | 30/70 [00:04<00:05,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  44%|██▋   | 31/70 [00:04<00:05,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|██▋   | 32/70 [00:04<00:05,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  47%|██▊   | 33/70 [00:04<00:05,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  49%|██▉   | 34/70 [00:05<00:05,  6.76it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|███   | 35/70 [00:05<00:05,  6.75it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  51%|███   | 36/70 [00:05<00:05,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  53%|███▏  | 37/70 [00:05<00:04,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|███▎  | 38/70 [00:05<00:04,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  56%|███▎  | 39/70 [00:05<00:04,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  57%|███▍  | 40/70 [00:05<00:04,  6.77it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001 | valid on 'valid' subset:  59%|███▌  | 41/70 [00:06<00:04,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|███▌  | 42/70 [00:06<00:04,  6.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  61%|███▋  | 43/70 [00:06<00:03,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  63%|███▊  | 44/70 [00:06<00:03,  6.77it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  64%|███▊  | 45/70 [00:06<00:03,  6.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  66%|███▉  | 46/70 [00:06<00:03,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  67%|████  | 47/70 [00:06<00:03,  6.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  69%|████  | 48/70 [00:07<00:03,  6.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  70%|████▏ | 49/70 [00:07<00:03,  6.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  71%|████▎ | 50/70 [00:07<00:02,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  73%|████▎ | 51/70 [00:07<00:02,  6.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  74%|████▍ | 52/70 [00:07<00:02,  6.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  76%|████▌ | 53/70 [00:07<00:02,  6.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  77%|████▋ | 54/70 [00:07<00:02,  6.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  79%|████▋ | 55/70 [00:08<00:02,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  80%|████▊ | 56/70 [00:08<00:02,  6.79it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  81%|████▉ | 57/70 [00:08<00:01,  6.80it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  83%|████▉ | 58/70 [00:08<00:01,  6.81it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  84%|█████ | 59/70 [00:08<00:01,  6.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  86%|█████▏| 60/70 [00:08<00:01,  6.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  87%|█████▏| 61/70 [00:09<00:01,  6.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  89%|█████▎| 62/70 [00:09<00:01,  6.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  90%|█████▍| 63/70 [00:09<00:01,  6.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  91%|█████▍| 64/70 [00:09<00:00,  6.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  93%|█████▌| 65/70 [00:09<00:00,  6.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  94%|█████▋| 66/70 [00:09<00:00,  6.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  96%|█████▋| 67/70 [00:09<00:00,  6.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  97%|█████▊| 68/70 [00:10<00:00,  6.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  99%|█████▉| 69/70 [00:10<00:00,  6.82it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|██████| 70/70 [00:10<00:00,  6.83it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 13:21:29 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.971 | ppl 125.42 | wps 20861.8 | wpb 3065.3 | bsz 1 | num_updates 11201\n",
      "2023-01-08 13:21:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 11201 updates\n",
      "2023-01-08 13:21:29 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-AdT/checkpoint1.pt\n",
      "2023-01-08 13:21:31 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-AdT/checkpoint1.pt\n",
      "2023-01-08 13:21:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-AdT/checkpoint1.pt (epoch 1 @ 11201 updates, score 6.971) (writing took 3.0144702980760485 seconds)\n",
      "2023-01-08 13:21:32 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-01-08 13:21:32 | INFO | train | epoch 001 | loss 8.854 | ppl 462.88 | wps 7034.4 | ups 0.76 | wpb 9215.6 | bsz 3 | num_updates 11201 | lr 0.700063 | gnorm 2.24 | clip 100 | train_wall 14624 | gb_free 6.6 | wall 14686\n",
      "2023-01-08 13:21:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 13:21:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 11201\n",
      "epoch 002:   0%|                                      | 0/11201 [00:00<?, ?it/s]2023-01-08 13:21:32 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-01-08 13:21:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:   2%| | 253/11201 [05:31<3:59:22,  1.31s/it, loss=7.626, ppl=197.56, ^C\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train --task language_modeling \\\n",
    "    data-bin/wikitext-103 \\\n",
    "    --save-dir checkpoints/transformer_wikitext-103-AdT \\\n",
    "    --arch transformer_lm_wiki103 \\\n",
    "    --max-update 286000 --lr 1.0 --t-mult 2 --lr-period-updates 270000 --lr-scheduler cosine --lr-shrink 0.75 \\\n",
    "    --warmup-updates 16000 --warmup-init-lr 1e-07 --stop-min-lr 1e-09 --optimizer nag --min-lr 0.0001 --clip-norm 0.1 \\\n",
    "    --criterion adaptive_loss --max-tokens 3072 --update-freq 3 --tokens-per-sample 3072 --seed 1 \\\n",
    "    --sample-break-mode none --skip-invalid-size-inputs-valid-test --ddp-backend=legacy_ddp \\\n",
    "    --wandb-project  NLP_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3005841b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 13:27:19 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-01-08 13:27:21 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'NLP_test', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0008], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/transformer_wikitext-103-mt2048-lr8', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0008]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0008]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 13:27:21 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types\n",
      "2023-01-08 13:27:24 | INFO | fairseq_cli.train | TransformerLanguageModel(\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(267744, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=267744, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-01-08 13:27:24 | INFO | fairseq_cli.train | task: LanguageModelingTask\n",
      "2023-01-08 13:27:24 | INFO | fairseq_cli.train | model: TransformerLanguageModel\n",
      "2023-01-08 13:27:24 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
      "2023-01-08 13:27:24 | INFO | fairseq_cli.train | num. shared model params: 155,999,232 (num. trained: 155,999,232)\n",
      "2023-01-08 13:27:24 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-01-08 13:27:24 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103/valid\n",
      "2023-01-08 13:27:25 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-01-08 13:27:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 13:27:25 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 \n",
      "2023-01-08 13:27:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 13:27:25 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-01-08 13:27:25 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None\n",
      "2023-01-08 13:27:25 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint_last.pt\n",
      "2023-01-08 13:27:25 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint_last.pt\n",
      "2023-01-08 13:27:25 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-01-08 13:27:25 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103/train\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 13:27:25 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 13:27:25 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 13:27:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001:   0%|                                       | 0/3151 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzh110\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/wandb/run-20230108_132727-3smity1j\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtransformer_wikitext-103-mt2048-lr8\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test/runs/3smity1j\u001b[0m\n",
      "2023-01-08 13:27:34 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-01-08 13:27:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-01-08 13:27:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                             | 1/3151 [00:11<9:52:05, 11.28s/it]2023-01-08 13:27:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   0%|                             | 2/3151 [00:13<5:06:22,  5.84s/it]2023-01-08 13:27:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   0%|                             | 3/3151 [00:16<4:13:09,  4.83s/it]2023-01-08 13:27:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:  99%|▉| 3111/3151 [1:41:42<01:18,  1.96s/it, loss=6.672, ppl=101.96, 2023-01-08 15:09:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001: 100%|▉| 3150/3151 [1:42:59<00:01,  1.96s/it, loss=6.672, ppl=101.96, 2023-01-08 15:10:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 15:10:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   2%|      | 2/106 [00:00<00:05, 17.98it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:05, 19.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   8%|▍     | 8/106 [00:00<00:04, 19.73it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:04, 19.94it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  13%|▋    | 14/106 [00:00<00:04, 20.63it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:04, 20.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:04, 20.51it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  22%|█    | 23/106 [00:01<00:04, 20.42it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  25%|█▏   | 26/106 [00:01<00:03, 20.44it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:01<00:03, 20.47it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  30%|█▌   | 32/106 [00:01<00:03, 20.13it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:01<00:03, 20.05it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  36%|█▊   | 38/106 [00:01<00:03, 19.95it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:02<00:03, 19.99it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  41%|██   | 43/106 [00:02<00:03, 19.94it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  43%|██▏  | 46/106 [00:02<00:02, 20.24it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|██▎  | 49/106 [00:02<00:02, 21.25it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  49%|██▍  | 52/106 [00:02<00:02, 21.02it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  52%|██▌  | 55/106 [00:02<00:02, 20.50it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  55%|██▋  | 58/106 [00:02<00:02, 20.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  58%|██▉  | 61/106 [00:03<00:02, 20.32it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  60%|███  | 64/106 [00:03<00:02, 20.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  63%|███▏ | 67/106 [00:03<00:01, 20.20it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  66%|███▎ | 70/106 [00:03<00:01, 20.41it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  69%|███▍ | 73/106 [00:03<00:01, 20.20it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  72%|███▌ | 76/106 [00:03<00:01, 20.11it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|███▋ | 79/106 [00:03<00:01, 19.97it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  76%|███▊ | 81/106 [00:04<00:01, 19.96it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  79%|███▉ | 84/106 [00:04<00:01, 20.86it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  82%|████ | 87/106 [00:04<00:00, 20.58it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  85%|████▏| 90/106 [00:04<00:00, 20.49it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|████▍| 93/106 [00:04<00:00, 20.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  91%|████▌| 96/106 [00:04<00:00, 20.34it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  93%|████▋| 99/106 [00:04<00:00, 20.09it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  96%|███▊| 102/106 [00:05<00:00, 19.96it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  98%|███▉| 104/106 [00:05<00:00, 19.90it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset: 100%|████| 106/106 [00:05<00:00, 19.85it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 15:10:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.378 | ppl 83.18 | wps 41190.2 | wpb 2043.6 | bsz 4 | num_updates 3146\n",
      "2023-01-08 15:10:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3146 updates\n",
      "2023-01-08 15:10:30 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint1.pt\n",
      "2023-01-08 15:10:33 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint1.pt\n",
      "2023-01-08 15:10:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint1.pt (epoch 1 @ 3146 updates, score 6.378) (writing took 3.4620857459958643 seconds)\n",
      "2023-01-08 15:10:34 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-01-08 15:10:34 | INFO | train | epoch 001 | loss 8.394 | ppl 336.45 | wps 16716.5 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 3146 | lr 0.000629221 | gnorm 0.861 | loss_scale 32 | train_wall 6143 | gb_free 15.4 | wall 6189\n",
      "2023-01-08 15:10:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 15:10:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 002:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 15:10:34 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-01-08 15:10:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  12%| | 374/3151 [12:07<1:30:37,  1.96s/it, loss=6.49, ppl=89.88, wps2023-01-08 15:22:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 002:  63%|▋| 1996/3151 [1:05:02<37:48,  1.96s/it, loss=6.07, ppl=67.17, wp2023-01-08 16:15:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 002:  97%|▉| 3069/3151 [1:40:01<02:40,  1.96s/it, loss=5.913, ppl=60.25, w2023-01-08 16:50:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 002: 100%|▉| 3150/3151 [1:42:40<00:01,  1.96s/it, loss=5.913, ppl=60.25, w2023-01-08 16:53:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 16:53:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   2%|      | 2/106 [00:00<00:05, 18.61it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:05, 19.38it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   7%|▍     | 7/106 [00:00<00:05, 19.46it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 002 | valid on 'valid' subset:   9%|▍    | 10/106 [00:00<00:04, 19.99it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  12%|▌    | 13/106 [00:00<00:04, 20.90it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  15%|▊    | 16/106 [00:00<00:04, 20.53it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  18%|▉    | 19/106 [00:00<00:04, 20.44it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  21%|█    | 22/106 [00:01<00:04, 20.31it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  24%|█▏   | 25/106 [00:01<00:03, 20.26it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  26%|█▎   | 28/106 [00:01<00:03, 20.37it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  29%|█▍   | 31/106 [00:01<00:03, 20.18it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  32%|█▌   | 34/106 [00:01<00:03, 20.21it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  35%|█▋   | 37/106 [00:01<00:03, 20.18it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|█▉   | 40/106 [00:01<00:03, 20.32it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  41%|██   | 43/106 [00:02<00:03, 20.24it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  43%|██▏  | 46/106 [00:02<00:02, 21.21it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|██▎  | 49/106 [00:02<00:02, 20.72it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  49%|██▍  | 52/106 [00:02<00:02, 20.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  52%|██▌  | 55/106 [00:02<00:02, 20.35it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  55%|██▋  | 58/106 [00:02<00:02, 20.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  58%|██▉  | 61/106 [00:02<00:02, 20.45it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  60%|███  | 64/106 [00:03<00:02, 20.26it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  63%|███▏ | 67/106 [00:03<00:01, 20.24it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  66%|███▎ | 70/106 [00:03<00:01, 20.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  69%|███▍ | 73/106 [00:03<00:01, 20.33it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  72%|███▌ | 76/106 [00:03<00:01, 20.26it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|███▋ | 79/106 [00:03<00:01, 20.31it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  77%|███▊ | 82/106 [00:04<00:01, 21.31it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  80%|████ | 85/106 [00:04<00:01, 20.94it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  83%|████▏| 88/106 [00:04<00:00, 20.57it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  86%|████▎| 91/106 [00:04<00:00, 20.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  89%|████▍| 94/106 [00:04<00:00, 20.44it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|████▌| 97/106 [00:04<00:00, 20.56it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  94%|███▊| 100/106 [00:04<00:00, 20.46it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  97%|███▉| 103/106 [00:05<00:00, 20.26it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset: 100%|████| 106/106 [00:05<00:00, 20.35it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 16:53:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.636 | ppl 49.74 | wps 41515.6 | wpb 2043.6 | bsz 4 | num_updates 6294 | best_loss 5.636\n",
      "2023-01-08 16:53:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 6294 updates\n",
      "2023-01-08 16:53:20 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint2.pt\n",
      "2023-01-08 16:53:22 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint2.pt\n",
      "2023-01-08 16:53:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint2.pt (epoch 2 @ 6294 updates, score 5.636) (writing took 4.974756079958752 seconds)\n",
      "2023-01-08 16:53:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-01-08 16:53:25 | INFO | train | epoch 002 | loss 6.182 | ppl 72.63 | wps 16711.7 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 6294 | lr 0.000637759 | gnorm 0.57 | loss_scale 16 | train_wall 6133 | gb_free 15.4 | wall 12360\n",
      "2023-01-08 16:53:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 16:53:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 003:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 16:53:25 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-01-08 16:53:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  37%|▎| 1170/3151 [37:59<1:04:30,  1.95s/it, loss=5.743, ppl=53.57, w2023-01-08 17:31:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 003:  70%|▋| 2206/3151 [1:11:42<30:45,  1.95s/it, loss=5.67, ppl=50.91, wp2023-01-08 18:05:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 003: 100%|▉| 3150/3151 [1:42:25<00:01,  1.95s/it, loss=5.634, ppl=49.65, w2023-01-08 18:35:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 18:35:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   2%|      | 2/106 [00:00<00:06, 16.40it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:05, 19.00it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   7%|▍     | 7/106 [00:00<00:05, 19.28it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   9%|▍    | 10/106 [00:00<00:04, 20.06it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  12%|▌    | 13/106 [00:00<00:04, 20.24it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|▊    | 16/106 [00:00<00:04, 20.09it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  18%|▉    | 19/106 [00:00<00:04, 20.59it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  21%|█    | 22/106 [00:01<00:03, 21.55it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  24%|█▏   | 25/106 [00:01<00:03, 21.16it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  26%|█▎   | 28/106 [00:01<00:03, 20.96it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  29%|█▍   | 31/106 [00:01<00:03, 20.99it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  32%|█▌   | 34/106 [00:01<00:03, 20.83it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  35%|█▋   | 37/106 [00:01<00:03, 20.53it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|█▉   | 40/106 [00:01<00:03, 20.67it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  41%|██   | 43/106 [00:02<00:03, 20.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  43%|██▏  | 46/106 [00:02<00:02, 20.55it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  46%|██▎  | 49/106 [00:02<00:02, 20.51it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  49%|██▍  | 52/106 [00:02<00:02, 20.63it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  52%|██▌  | 55/106 [00:02<00:02, 20.72it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  55%|██▋  | 58/106 [00:02<00:02, 21.34it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  58%|██▉  | 61/106 [00:02<00:02, 21.16it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  60%|███  | 64/106 [00:03<00:02, 20.76it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  63%|███▏ | 67/106 [00:03<00:01, 20.84it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  66%|███▎ | 70/106 [00:03<00:01, 20.74it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  69%|███▍ | 73/106 [00:03<00:01, 20.84it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  72%|███▌ | 76/106 [00:03<00:01, 20.71it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|███▋ | 79/106 [00:03<00:01, 20.57it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|███▊ | 82/106 [00:03<00:01, 20.46it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  80%|████ | 85/106 [00:04<00:01, 20.26it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  83%|████▏| 88/106 [00:04<00:00, 20.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  86%|████▎| 91/106 [00:04<00:00, 21.31it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  89%|████▍| 94/106 [00:04<00:00, 21.61it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|████▌| 97/106 [00:04<00:00, 21.14it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  94%|███▊| 100/106 [00:04<00:00, 20.98it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  97%|███▉| 103/106 [00:04<00:00, 20.70it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████| 106/106 [00:05<00:00, 20.44it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 18:35:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.392 | ppl 42 | wps 42190.5 | wpb 2043.6 | bsz 4 | num_updates 9443 | best_loss 5.392\n",
      "2023-01-08 18:35:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 9443 updates\n",
      "2023-01-08 18:35:56 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint3.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 18:35:58 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint3.pt\n",
      "2023-01-08 18:36:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint3.pt (epoch 3 @ 9443 updates, score 5.392) (writing took 4.572182341944426 seconds)\n",
      "2023-01-08 18:36:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-01-08 18:36:01 | INFO | train | epoch 003 | loss 5.707 | ppl 52.24 | wps 16757.3 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 9443 | lr 0.000520673 | gnorm 0.522 | loss_scale 16 | train_wall 6112 | gb_free 15.4 | wall 18516\n",
      "2023-01-08 18:36:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 18:36:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 004:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 18:36:01 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-01-08 18:36:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:   3%| | 87/3151 [02:42<1:39:27,  1.95s/it, loss=5.58, ppl=47.84, wps=2023-01-08 18:38:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  36%|▎| 1128/3151 [36:31<1:05:46,  1.95s/it, loss=5.501, ppl=45.29, w2023-01-08 19:12:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  69%|▋| 2179/3151 [1:10:39<31:34,  1.95s/it, loss=5.507, ppl=45.47, w2023-01-08 19:46:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004: 100%|▉| 3150/3151 [1:42:12<00:01,  1.95s/it, loss=5.485, ppl=44.77, w2023-01-08 20:18:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 20:18:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   2%|      | 2/106 [00:00<00:05, 18.51it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:05, 19.90it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   8%|▍     | 8/106 [00:00<00:04, 20.18it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:04, 20.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  13%|▋    | 14/106 [00:00<00:04, 21.82it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:04, 21.49it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:04, 21.16it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  22%|█    | 23/106 [00:01<00:03, 20.97it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  25%|█▏   | 26/106 [00:01<00:03, 21.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:01<00:03, 20.85it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  30%|█▌   | 32/106 [00:01<00:03, 20.85it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:01<00:03, 20.81it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  36%|█▊   | 38/106 [00:01<00:03, 20.72it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:01<00:03, 20.83it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  42%|██   | 44/106 [00:02<00:02, 20.75it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:02<00:02, 20.57it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  47%|██▎  | 50/106 [00:02<00:02, 21.60it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:02<00:02, 21.44it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  53%|██▋  | 56/106 [00:02<00:02, 21.15it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:02<00:02, 20.96it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  58%|██▉  | 62/106 [00:02<00:02, 21.00it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  61%|███  | 65/106 [00:03<00:01, 20.84it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  64%|███▏ | 68/106 [00:03<00:01, 20.85it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:03<00:01, 20.78it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  70%|███▍ | 74/106 [00:03<00:01, 20.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:03<00:01, 20.83it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|███▊ | 80/106 [00:03<00:01, 20.74it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:03<00:01, 20.68it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  81%|████ | 86/106 [00:04<00:00, 21.57it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  84%|████▏| 89/106 [00:04<00:00, 21.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  87%|████▎| 92/106 [00:04<00:00, 21.16it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  90%|████▍| 95/106 [00:04<00:00, 20.88it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  92%|████▌| 98/106 [00:04<00:00, 20.92it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  95%|███▊| 101/106 [00:04<00:00, 20.82it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  98%|███▉| 104/106 [00:04<00:00, 20.91it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 20:18:19 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.261 | ppl 38.35 | wps 42556.3 | wpb 2043.6 | bsz 4 | num_updates 12591 | best_loss 5.261\n",
      "2023-01-08 20:18:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 12591 updates\n",
      "2023-01-08 20:18:19 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint4.pt\n",
      "2023-01-08 20:18:21 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint4.pt\n",
      "2023-01-08 20:18:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint4.pt (epoch 4 @ 12591 updates, score 5.261) (writing took 4.46954857208766 seconds)\n",
      "2023-01-08 20:18:23 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-01-08 20:18:23 | INFO | train | epoch 004 | loss 5.506 | ppl 45.43 | wps 16789.8 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 12591 | lr 0.00045091 | gnorm 0.529 | loss_scale 16 | train_wall 6098 | gb_free 15.4 | wall 24658\n",
      "2023-01-08 20:18:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 20:18:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 005:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 20:18:23 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-01-08 20:18:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:   2%| | 74/3151 [02:18<1:40:00,  1.95s/it, loss=5.461, ppl=44.06, wps2023-01-08 20:20:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005:  36%|▎| 1140/3151 [36:55<1:05:19,  1.95s/it, loss=5.387, ppl=41.83, w2023-01-08 20:55:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005:  72%|▋| 2254/3151 [1:13:22<29:21,  1.96s/it, loss=5.396, ppl=42.11, w2023-01-08 21:31:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005: 100%|▉| 3150/3151 [1:42:42<00:01,  1.96s/it, loss=5.404, ppl=42.33, w2023-01-08 22:01:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 22:01:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   2%|      | 2/106 [00:00<00:05, 18.46it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:05, 19.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▍     | 8/106 [00:00<00:04, 20.30it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:04, 20.44it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  13%|▋    | 14/106 [00:00<00:04, 20.68it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 005 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:04, 20.61it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:04, 20.46it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  22%|█    | 23/106 [00:01<00:04, 20.64it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  25%|█▏   | 26/106 [00:01<00:03, 20.58it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:01<00:03, 22.13it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  30%|█▌   | 32/106 [00:01<00:03, 21.58it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:01<00:03, 21.37it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  36%|█▊   | 38/106 [00:01<00:03, 21.17it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:01<00:03, 20.98it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  42%|██   | 44/106 [00:02<00:02, 20.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:02<00:02, 20.55it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  47%|██▎  | 50/106 [00:02<00:02, 20.79it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:02<00:02, 20.71it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  53%|██▋  | 56/106 [00:02<00:02, 20.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:02<00:02, 20.80it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  58%|██▉  | 62/106 [00:02<00:02, 20.68it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  61%|███  | 65/106 [00:03<00:01, 21.93it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  64%|███▏ | 68/106 [00:03<00:01, 21.34it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:03<00:01, 21.20it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  70%|███▍ | 74/106 [00:03<00:01, 20.97it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:03<00:01, 20.84it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|███▊ | 80/106 [00:03<00:01, 20.92it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:03<00:01, 20.69it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  81%|████ | 86/106 [00:04<00:00, 20.79it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  84%|████▏| 89/106 [00:04<00:00, 20.50it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  87%|████▎| 92/106 [00:04<00:00, 20.54it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  90%|████▍| 95/106 [00:04<00:00, 20.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|████▌| 98/106 [00:04<00:00, 20.62it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  95%|███▊| 101/106 [00:04<00:00, 22.10it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  98%|███▉| 104/106 [00:04<00:00, 21.56it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 22:01:11 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.199 | ppl 36.74 | wps 42621.8 | wpb 2043.6 | bsz 4 | num_updates 15739 | best_loss 5.199\n",
      "2023-01-08 22:01:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 15739 updates\n",
      "2023-01-08 22:01:11 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint5.pt\n",
      "2023-01-08 22:01:13 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint5.pt\n",
      "2023-01-08 22:01:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint5.pt (epoch 5 @ 15739 updates, score 5.199) (writing took 4.553269885946065 seconds)\n",
      "2023-01-08 22:01:15 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-01-08 22:01:15 | INFO | train | epoch 005 | loss 5.389 | ppl 41.91 | wps 16708.2 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 15739 | lr 0.000403303 | gnorm 0.547 | loss_scale 16 | train_wall 6128 | gb_free 15.4 | wall 30831\n",
      "2023-01-08 22:01:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 22:01:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 006:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 22:01:15 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-01-08 22:01:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:   8%| | 253/3151 [08:07<1:34:09,  1.95s/it, loss=5.262, ppl=38.38, wp2023-01-08 22:09:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006:  44%|▍| 1374/3151 [44:31<57:43,  1.95s/it, loss=5.309, ppl=39.63, wps2023-01-08 22:45:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006:  79%|▊| 2477/3151 [1:20:21<21:54,  1.95s/it, loss=5.329, ppl=40.19, w2023-01-08 23:21:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006: 100%|▉| 3150/3151 [1:42:13<00:01,  1.95s/it, loss=5.325, ppl=40.08, w2023-01-08 23:43:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 23:43:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   2%|      | 2/106 [00:00<00:05, 18.18it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:05, 20.06it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   8%|▍     | 8/106 [00:00<00:04, 20.30it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:04, 20.37it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  13%|▋    | 14/106 [00:00<00:04, 20.12it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:04, 20.30it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:04, 20.54it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  22%|█    | 23/106 [00:01<00:04, 20.56it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  25%|█▏   | 26/106 [00:01<00:03, 22.51it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:01<00:03, 21.95it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  30%|█▌   | 32/106 [00:01<00:03, 21.50it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:01<00:03, 21.38it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  36%|█▊   | 38/106 [00:01<00:03, 20.97it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:01<00:03, 20.98it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  42%|██   | 44/106 [00:02<00:02, 20.85it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:02<00:02, 20.77it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  47%|██▎  | 50/106 [00:02<00:02, 20.87it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:02<00:02, 20.74it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  53%|██▋  | 56/106 [00:02<00:02, 20.79it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:02<00:02, 20.71it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  58%|██▉  | 62/106 [00:02<00:02, 21.85it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  61%|███  | 65/106 [00:03<00:01, 21.80it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  64%|███▏ | 68/106 [00:03<00:01, 21.42it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:03<00:01, 21.39it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  70%|███▍ | 74/106 [00:03<00:01, 21.13it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:03<00:01, 20.95it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|███▊ | 80/106 [00:03<00:01, 20.94it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:03<00:01, 20.80it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  81%|████ | 86/106 [00:04<00:00, 20.85it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  84%|████▏| 89/106 [00:04<00:00, 20.74it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  87%|████▎| 92/106 [00:04<00:00, 20.69it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  90%|████▍| 95/106 [00:04<00:00, 20.81it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|████▌| 98/106 [00:04<00:00, 21.31it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  95%|███▊| 101/106 [00:04<00:00, 21.76it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  98%|███▉| 104/106 [00:04<00:00, 21.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 23:43:34 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.148 | ppl 35.47 | wps 42784.1 | wpb 2043.6 | bsz 4 | num_updates 18887 | best_loss 5.148\n",
      "2023-01-08 23:43:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 18887 updates\n",
      "2023-01-08 23:43:34 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint6.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 23:43:36 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint6.pt\n",
      "2023-01-08 23:43:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr8/checkpoint6.pt (epoch 6 @ 18887 updates, score 5.148) (writing took 4.588257367955521 seconds)\n",
      "2023-01-08 23:43:39 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-01-08 23:43:39 | INFO | train | epoch 006 | loss 5.311 | ppl 39.7 | wps 16786.4 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 18887 | lr 0.000368162 | gnorm 0.568 | loss_scale 16 | train_wall 6099 | gb_free 15.4 | wall 36974\n",
      "2023-01-08 23:43:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 23:43:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 007:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 23:43:39 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-01-08 23:43:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:  15%|▏| 460/3151 [14:50<1:27:22,  1.95s/it, loss=5.212, ppl=37.08, wp2023-01-08 23:58:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007:  18%|▏| 553/3151 [17:51<1:24:27,  1.95s/it, loss=5.225, ppl=37.4, wps^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 574, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 205, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 331, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/trainer.py\", line 843, in train_step\n",
      "    loss, sample_size_i, logging_output = self.task.train_step(\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/tasks/fairseq_task.py\", line 531, in train_step\n",
      "    loss, sample_size, logging_output = criterion(model, sample)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/criterions/cross_entropy.py\", line 35, in forward\n",
      "    net_output = model(**sample[\"net_input\"])\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/fairseq_model.py\", line 506, in forward\n",
      "    return self.decoder(src_tokens, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
      "    x, extra = self.extract_features(\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
      "    return self.extract_features_scriptable(\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 328, in extract_features_scriptable\n",
      "    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():\n",
      "KeyboardInterrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train --task language_modeling \\\n",
    "  data-bin/wikitext-103 \\\n",
    "  --save-dir checkpoints/transformer_wikitext-103-mt2048-lr8 \\\n",
    "  --arch transformer_lm --share-decoder-input-output-embed \\\n",
    "  --dropout 0.1 \\\n",
    "  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\n",
    "  --lr 0.0008 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "  --tokens-per-sample 512 --sample-break-mode none \\\n",
    "  --max-tokens 2048 --update-freq 16 \\\n",
    "  --fp16 \\\n",
    "  --max-update 50000 \\\n",
    "  --wandb-project  NLP_test "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GALAI",
   "language": "python",
   "name": "galai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
