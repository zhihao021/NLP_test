{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7afc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq\n"
     ]
    }
   ],
   "source": [
    "cd ../fairseq/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d736d995",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81a5c220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433c2ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version      Editable project location\n",
      "------------------------ ------------ --------------------------------------------------------------\n",
      "absl-py                  1.3.0\n",
      "accelerate               0.14.0\n",
      "aiohttp                  3.8.3\n",
      "aiosignal                1.3.1\n",
      "altair                   4.2.0\n",
      "antlr4-python3-runtime   4.8\n",
      "anyio                    3.6.2\n",
      "argon2-cffi              21.3.0\n",
      "argon2-cffi-bindings     21.2.0\n",
      "astor                    0.8.1\n",
      "asttokens                2.1.0\n",
      "async-timeout            4.0.2\n",
      "attrs                    22.1.0\n",
      "backcall                 0.2.0\n",
      "backports.zoneinfo       0.2.1\n",
      "base58                   2.1.1\n",
      "beautifulsoup4           4.11.1\n",
      "bert-score               0.3.12\n",
      "bitarray                 2.6.2\n",
      "black                    21.12b0\n",
      "bleach                   5.0.1\n",
      "blinker                  1.5\n",
      "blis                     0.7.9\n",
      "Brotli                   1.0.9\n",
      "brotlipy                 0.7.0\n",
      "cachetools               5.2.0\n",
      "catalogue                2.0.8\n",
      "certifi                  2022.9.24\n",
      "cffi                     1.15.0\n",
      "charset-normalizer       2.1.1\n",
      "click                    7.1.2\n",
      "colorama                 0.4.6\n",
      "contourpy                1.0.6\n",
      "cryptography             37.0.2\n",
      "cycler                   0.11.0\n",
      "cymem                    2.0.7\n",
      "Cython                   0.29.33\n",
      "dacite                   1.6.0\n",
      "datasets                 2.7.1\n",
      "debugpy                  1.6.3\n",
      "decorator                5.1.1\n",
      "defusedxml               0.7.1\n",
      "dill                     0.3.6\n",
      "entrypoints              0.4\n",
      "et-xmlfile               1.1.0\n",
      "exceptiongroup           1.0.4\n",
      "executing                1.2.0\n",
      "fairseq                  0.12.2       /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq\n",
      "fastBPE                  0.1.0\n",
      "fastjsonschema           2.16.2\n",
      "filelock                 3.8.0\n",
      "flake8                   5.0.4\n",
      "fonttools                4.38.0\n",
      "frozenlist               1.3.3\n",
      "fsspec                   2022.11.0\n",
      "galai                    1.0.1\n",
      "gitdb                    4.0.9\n",
      "GitPython                3.1.29\n",
      "huggingface-hub          0.11.0\n",
      "hydra-core               1.0.7\n",
      "idna                     3.4\n",
      "importlib-metadata       5.0.0\n",
      "importlib-resources      5.10.0\n",
      "inflate64                0.3.1\n",
      "iniconfig                1.1.1\n",
      "ipykernel                6.17.1\n",
      "ipython                  8.6.0\n",
      "ipython-genutils         0.2.0\n",
      "ipywidgets               8.0.2\n",
      "isort                    5.8.0\n",
      "jedi                     0.18.2\n",
      "Jinja2                   3.1.2\n",
      "joblib                   1.2.0\n",
      "jsonschema               4.17.1\n",
      "jupyter                  1.0.0\n",
      "jupyter_client           7.4.7\n",
      "jupyter-console          6.4.4\n",
      "jupyter_core             5.0.0\n",
      "jupyter-server           1.23.3\n",
      "jupyterlab-pygments      0.2.2\n",
      "jupyterlab-widgets       3.0.3\n",
      "kiwisolver               1.4.4\n",
      "langcodes                3.3.0\n",
      "lxml                     4.9.2\n",
      "MarkupSafe               2.1.1\n",
      "matplotlib               3.6.2\n",
      "matplotlib-inline        0.1.6\n",
      "mccabe                   0.7.0\n",
      "mistune                  2.0.4\n",
      "mkl-fft                  1.3.1\n",
      "mkl-random               1.2.2\n",
      "mkl-service              2.4.0\n",
      "more-itertools           9.0.0\n",
      "multidict                6.0.2\n",
      "multiprocess             0.70.14\n",
      "multivolumefile          0.2.3\n",
      "murmurhash               1.0.9\n",
      "mypy-extensions          0.4.3\n",
      "nbclassic                0.4.8\n",
      "nbclient                 0.7.0\n",
      "nbconvert                7.2.5\n",
      "nbformat                 5.7.0\n",
      "nest-asyncio             1.5.6\n",
      "nltk                     3.7\n",
      "notebook                 6.5.2\n",
      "notebook_shim            0.2.2\n",
      "numpy                    1.23.4\n",
      "nvidia-cublas-cu11       11.10.3.66\n",
      "nvidia-cuda-nvrtc-cu11   11.7.99\n",
      "nvidia-cuda-runtime-cu11 11.7.99\n",
      "nvidia-cudnn-cu11        8.5.0.96\n",
      "olefile                  0.46\n",
      "omegaconf                2.0.6\n",
      "openai                   0.25.0\n",
      "openpyxl                 3.0.10\n",
      "packaging                21.3\n",
      "pandas                   1.5.1\n",
      "pandas-stubs             1.5.1.221024\n",
      "pandocfilters            1.5.0\n",
      "parallelformers          1.2.7\n",
      "parso                    0.8.3\n",
      "pathspec                 0.10.2\n",
      "pathy                    0.9.0\n",
      "pexpect                  4.8.0\n",
      "pickleshare              0.7.5\n",
      "Pillow                   8.2.0\n",
      "pip                      22.2.2\n",
      "pkgutil_resolve_name     1.3.10\n",
      "platformdirs             2.5.4\n",
      "plotly                   5.11.0\n",
      "pluggy                   1.0.0\n",
      "portalocker              2.6.0\n",
      "preshed                  3.0.8\n",
      "prometheus-client        0.15.0\n",
      "prompt-toolkit           3.0.33\n",
      "promptsource             0.2.3\n",
      "protobuf                 4.21.9\n",
      "psutil                   5.9.4\n",
      "ptyprocess               0.7.0\n",
      "pure-eval                0.2.2\n",
      "py7zr                    0.20.2\n",
      "pyarrow                  10.0.1\n",
      "pybcj                    1.0.1\n",
      "pycodestyle              2.9.1\n",
      "pycparser                2.21\n",
      "pycryptodomex            3.15.0\n",
      "pydantic                 1.8.2\n",
      "pydeck                   0.8.0\n",
      "pyflakes                 2.5.0\n",
      "Pygments                 2.13.0\n",
      "pyOpenSSL                22.0.0\n",
      "pyparsing                3.0.9\n",
      "pyppmd                   1.0.0\n",
      "pyrsistent               0.19.2\n",
      "PySocks                  1.7.1\n",
      "pytest                   7.2.0\n",
      "python-dateutil          2.8.2\n",
      "pytz                     2022.6\n",
      "pytz-deprecation-shim    0.1.0.post0\n",
      "PyYAML                   6.0\n",
      "pyzmq                    24.0.1\n",
      "pyzstd                   0.15.3\n",
      "qtconsole                5.4.0\n",
      "QtPy                     2.3.0\n",
      "regex                    2022.10.31\n",
      "requests                 2.28.1\n",
      "responses                0.18.0\n",
      "rouge-score              0.1.2\n",
      "sacrebleu                2.3.1\n",
      "sacremoses               0.0.53\n",
      "scikit-learn             1.2.0\n",
      "scipy                    1.10.0\n",
      "Send2Trash               1.8.0\n",
      "setuptools               65.5.0\n",
      "six                      1.16.0\n",
      "smart-open               5.2.1\n",
      "smmap                    5.0.0\n",
      "sniffio                  1.3.0\n",
      "soupsieve                2.3.2.post1\n",
      "spacy                    3.3.0\n",
      "spacy-legacy             3.0.10\n",
      "spacy-loggers            1.0.3\n",
      "srsly                    2.4.5\n",
      "stack-data               0.6.1\n",
      "streamlit                0.82.0\n",
      "tabulate                 0.9.0\n",
      "tenacity                 8.1.0\n",
      "terminado                0.17.0\n",
      "texttable                1.6.6\n",
      "thefuzz                  0.19.0\n",
      "thinc                    8.0.17\n",
      "threadpoolctl            3.1.0\n",
      "tinycss2                 1.2.1\n",
      "tokenizers               0.13.2\n",
      "toml                     0.10.2\n",
      "tomli                    1.2.3\n",
      "toolz                    0.12.0\n",
      "torch                    1.13.1\n",
      "torchaudio               0.12.0\n",
      "torchvision              0.13.0\n",
      "tornado                  6.2\n",
      "tqdm                     4.64.1\n",
      "traitlets                5.5.0\n",
      "transformers             4.24.0\n",
      "typer                    0.4.2\n",
      "types-pytz               2022.6.0.1\n",
      "typing_extensions        4.4.0\n",
      "tzdata                   2022.6\n",
      "tzlocal                  4.2\n",
      "urllib3                  1.26.11\n",
      "validators               0.20.0\n",
      "wasabi                   0.10.1\n",
      "watchdog                 2.1.9\n",
      "wcwidth                  0.2.5\n",
      "webencodings             0.5.1\n",
      "websocket-client         1.4.2\n",
      "wheel                    0.37.1\n",
      "widgetsnbextension       4.0.3\n",
      "xxhash                   3.1.0\n",
      "yarl                     1.8.1\n",
      "zipp                     3.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ebb5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-07 20:09:18 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-01-07 20:09:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'NLP_test', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 1024, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 1024, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/transformer_wikitext-103', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-07 20:09:20 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types\n",
      "2023-01-07 20:09:22 | INFO | fairseq_cli.train | TransformerLanguageModel(\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(267744, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=267744, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-01-07 20:09:22 | INFO | fairseq_cli.train | task: LanguageModelingTask\n",
      "2023-01-07 20:09:22 | INFO | fairseq_cli.train | model: TransformerLanguageModel\n",
      "2023-01-07 20:09:22 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
      "2023-01-07 20:09:22 | INFO | fairseq_cli.train | num. shared model params: 155,999,232 (num. trained: 155,999,232)\n",
      "2023-01-07 20:09:22 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-01-07 20:09:22 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103/valid\n",
      "2023-01-07 20:09:23 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-01-07 20:09:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-07 20:09:23 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 \n",
      "2023-01-07 20:09:23 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-07 20:09:23 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-01-07 20:09:23 | INFO | fairseq_cli.train | max tokens per device = 1024 and max sentences per device = None\n",
      "2023-01-07 20:09:23 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/transformer_wikitext-103/checkpoint_last.pt\n",
      "2023-01-07 20:09:23 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/transformer_wikitext-103/checkpoint_last.pt\n",
      "2023-01-07 20:09:23 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-01-07 20:09:23 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103/train\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-07 20:09:23 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-07 20:09:23 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-07 20:09:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001:   0%|                                       | 0/6301 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzh110\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/wandb/run-20230107_200925-11k50qn1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtransformer_wikitext-103\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test/runs/11k50qn1\u001b[0m\n",
      "2023-01-07 20:09:31 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-01-07 20:09:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-01-07 20:09:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                            | 1/6301 [00:10<17:48:21, 10.17s/it]2023-01-07 20:09:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   0%|                             | 2/6301 [00:11<8:45:03,  5.00s/it]2023-01-07 20:09:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:  49%|▍| 3113/6301 [27:07<27:35,  1.93it/s, loss=7.349, ppl=163.02, wp2023-01-07 20:36:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:  66%|▋| 4139/6301 [35:58<18:38,  1.93it/s, loss=6.966, ppl=125.05, wp2023-01-07 20:45:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:  68%|▋| 4287/6301 [37:14<17:22,  1.93it/s, loss=6.905, ppl=119.85, wp2023-01-07 20:46:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:  86%|▊| 5431/6301 [47:06<07:27,  1.94it/s, loss=6.598, ppl=96.88, wps2023-01-07 20:56:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001: 100%|▉| 6300/6301 [54:33<00:00,  1.94it/s, loss=6.477, ppl=89.06, wps2023-01-07 21:03:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-07 21:03:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|              | 0/212 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   3%|▏     | 7/212 [00:00<00:02, 69.89it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   8%|▍    | 16/212 [00:00<00:02, 78.57it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  12%|▌    | 25/212 [00:00<00:02, 81.29it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  16%|▊    | 34/212 [00:00<00:02, 82.55it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  20%|█    | 43/212 [00:00<00:02, 83.21it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  25%|█▏   | 52/212 [00:00<00:01, 83.68it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  29%|█▍   | 61/212 [00:00<00:01, 83.83it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  33%|█▋   | 70/212 [00:00<00:01, 83.75it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  37%|█▊   | 79/212 [00:00<00:01, 83.94it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  42%|██   | 88/212 [00:01<00:01, 84.18it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  46%|██▎  | 97/212 [00:01<00:01, 84.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|██  | 106/212 [00:01<00:01, 84.16it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  54%|██▏ | 115/212 [00:01<00:01, 84.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  58%|██▎ | 124/212 [00:01<00:01, 84.35it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  63%|██▌ | 133/212 [00:01<00:00, 84.40it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  67%|██▋ | 142/212 [00:01<00:00, 84.44it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  71%|██▊ | 151/212 [00:01<00:00, 84.37it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|███ | 160/212 [00:01<00:00, 84.44it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  80%|███▏| 169/212 [00:02<00:00, 84.50it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  84%|███▎| 178/212 [00:02<00:00, 84.48it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  88%|███▌| 187/212 [00:02<00:00, 84.36it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|███▋| 196/212 [00:02<00:00, 83.92it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  97%|███▊| 205/212 [00:02<00:00, 85.48it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-07 21:04:00 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.145 | ppl 70.78 | wps 86481.1 | wpb 1021.8 | bsz 2 | num_updates 6294\n",
      "2023-01-07 21:04:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 6294 updates\n",
      "2023-01-07 21:04:00 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint1.pt\n",
      "2023-01-07 21:04:02 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint1.pt\n",
      "2023-01-07 21:04:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103/checkpoint1.pt (epoch 1 @ 6294 updates, score 6.145) (writing took 3.1224978379905224 seconds)\n",
      "2023-01-07 21:04:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-01-07 21:04:03 | INFO | train | epoch 001 | loss 7.909 | ppl 240.33 | wps 31572.5 | ups 1.93 | wpb 16382.5 | bsz 32 | num_updates 6294 | lr 0.000398599 | gnorm 0.828 | loss_scale 32 | train_wall 3205 | gb_free 18.1 | wall 3280\n",
      "2023-01-07 21:04:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-07 21:04:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n",
      "epoch 002:   0%|                                       | 0/6301 [00:00<?, ?it/s]2023-01-07 21:04:03 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-01-07 21:04:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:   3%| | 196/6301 [01:41<52:30,  1.94it/s, loss=6.366, ppl=82.49, wps=2023-01-07 21:05:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 002:  19%|▏| 1222/6301 [10:31<43:44,  1.94it/s, loss=6.261, ppl=76.68, wps2023-01-07 21:14:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 002:  36%|▎| 2250/6301 [19:22<34:56,  1.93it/s, loss=6.167, ppl=71.85, wps2023-01-07 21:23:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 002:  53%|▌| 3319/6301 [28:37<25:38,  1.94it/s, loss=6.124, ppl=69.74, wps2023-01-07 21:32:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 002:  69%|▋| 4374/6301 [37:42<16:32,  1.94it/s, loss=6.043, ppl=65.95, wps2023-01-07 21:41:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 002:  86%|▊| 5423/6301 [46:42<07:32,  1.94it/s, loss=6.022, ppl=65, wps=312023-01-07 21:50:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 002: 100%|▉| 6300/6301 [54:14<00:00,  1.93it/s, loss=5.984, ppl=63.3, wps=2023-01-07 21:58:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-07 21:58:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|              | 0/212 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   3%|▏     | 7/212 [00:00<00:02, 68.62it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   8%|▍    | 16/212 [00:00<00:02, 77.64it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  12%|▌    | 25/212 [00:00<00:02, 80.52it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  16%|▊    | 34/212 [00:00<00:02, 82.04it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  20%|█    | 43/212 [00:00<00:02, 82.92it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 002 | valid on 'valid' subset:  25%|█▏   | 52/212 [00:00<00:01, 83.33it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  29%|█▍   | 61/212 [00:00<00:01, 83.67it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  33%|█▋   | 70/212 [00:00<00:01, 83.88it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  37%|█▊   | 79/212 [00:00<00:01, 84.06it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  42%|██   | 88/212 [00:01<00:01, 84.13it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  46%|██▎  | 97/212 [00:01<00:01, 84.21it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  50%|██  | 106/212 [00:01<00:01, 84.22it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  54%|██▏ | 115/212 [00:01<00:01, 84.37it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  58%|██▎ | 124/212 [00:01<00:01, 84.39it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  63%|██▌ | 133/212 [00:01<00:00, 84.40it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  67%|██▋ | 142/212 [00:01<00:00, 84.40it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  71%|██▊ | 151/212 [00:01<00:00, 84.18it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|███ | 160/212 [00:01<00:00, 84.08it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  80%|███▏| 169/212 [00:02<00:00, 83.93it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  84%|███▎| 178/212 [00:02<00:00, 84.07it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  88%|███▌| 187/212 [00:02<00:00, 84.13it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  92%|███▋| 196/212 [00:02<00:00, 84.13it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  97%|███▊| 205/212 [00:02<00:00, 85.78it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-07 21:58:20 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.667 | ppl 50.82 | wps 86370 | wpb 1021.8 | bsz 2 | num_updates 12589 | best_loss 5.667\n",
      "2023-01-07 21:58:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 12589 updates\n",
      "2023-01-07 21:58:20 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint2.pt\n",
      "2023-01-07 21:58:22 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint2.pt\n",
      "2023-01-07 21:58:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103/checkpoint2.pt (epoch 2 @ 12589 updates, score 5.667) (writing took 4.5458223529858515 seconds)\n",
      "2023-01-07 21:58:25 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-01-07 21:58:25 | INFO | train | epoch 002 | loss 6.137 | ppl 70.36 | wps 31617.7 | ups 1.93 | wpb 16382.5 | bsz 32 | num_updates 12589 | lr 0.000281841 | gnorm 0.71 | loss_scale 32 | train_wall 3193 | gb_free 18.1 | wall 6542\n",
      "2023-01-07 21:58:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-07 21:58:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n",
      "epoch 003:   0%|                                       | 0/6301 [00:00<?, ?it/s]2023-01-07 21:58:25 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-01-07 21:58:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:   3%| | 164/6301 [01:24<52:47,  1.94it/s, loss=5.866, ppl=58.33, wps=2023-01-07 21:59:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 003:  19%|▏| 1205/6301 [10:24<43:52,  1.94it/s, loss=5.891, ppl=59.33, wps2023-01-07 22:08:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 003:  24%|▏| 1506/6301 [13:00<41:22,  1.93it/s, loss=5.878, ppl=58.79, wps2023-01-07 22:11:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 003:  57%|▌| 3566/6301 [30:41<23:26,  1.94it/s, loss=5.801, ppl=55.74, wps2023-01-07 22:29:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 003:  73%|▋| 4591/6301 [39:28<14:42,  1.94it/s, loss=5.81, ppl=56.1, wps=32023-01-07 22:37:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 003:  89%|▉| 5616/6301 [48:18<05:54,  1.93it/s, loss=5.782, ppl=55.01, wps2023-01-07 22:46:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 003: 100%|▉| 6300/6301 [54:10<00:00,  1.94it/s, loss=5.799, ppl=55.67, wps2023-01-07 22:52:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-07 22:52:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|              | 0/212 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   3%|▏     | 6/212 [00:00<00:03, 54.02it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   7%|▎    | 15/212 [00:00<00:02, 71.38it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  11%|▌    | 24/212 [00:00<00:02, 77.30it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  16%|▊    | 33/212 [00:00<00:02, 80.21it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  20%|▉    | 42/212 [00:00<00:02, 81.81it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  24%|█▏   | 51/212 [00:00<00:01, 81.96it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  28%|█▍   | 60/212 [00:00<00:01, 82.41it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  33%|█▋   | 69/212 [00:00<00:01, 82.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  37%|█▊   | 78/212 [00:00<00:01, 83.24it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  41%|██   | 87/212 [00:01<00:01, 83.44it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  45%|██▎  | 96/212 [00:01<00:01, 83.27it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  50%|█▉  | 105/212 [00:01<00:01, 83.58it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  54%|██▏ | 114/212 [00:01<00:01, 83.80it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  58%|██▎ | 123/212 [00:01<00:01, 83.92it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  62%|██▍ | 132/212 [00:01<00:00, 83.97it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  67%|██▋ | 141/212 [00:01<00:00, 84.02it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  71%|██▊ | 150/212 [00:01<00:00, 84.12it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|███ | 159/212 [00:01<00:00, 84.10it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  79%|███▏| 168/212 [00:02<00:00, 84.16it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  83%|███▎| 177/212 [00:02<00:00, 83.60it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  88%|███▌| 186/212 [00:02<00:00, 83.76it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|███▋| 195/212 [00:02<00:00, 83.81it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  96%|███▊| 204/212 [00:02<00:00, 85.28it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-07 22:52:39 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.494 | ppl 45.07 | wps 86086.9 | wpb 1021.8 | bsz 2 | num_updates 18884 | best_loss 5.494\n",
      "2023-01-07 22:52:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 18884 updates\n",
      "2023-01-07 22:52:39 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint3.pt\n",
      "2023-01-07 22:52:40 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint3.pt\n",
      "2023-01-07 22:52:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103/checkpoint3.pt (epoch 3 @ 18884 updates, score 5.494) (writing took 4.584307975950651 seconds)\n",
      "2023-01-07 22:52:43 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-01-07 22:52:43 | INFO | train | epoch 003 | loss 5.833 | ppl 57.02 | wps 31650.1 | ups 1.93 | wpb 16382.5 | bsz 32 | num_updates 18884 | lr 0.000230119 | gnorm 0.76 | loss_scale 32 | train_wall 3189 | gb_free 18.1 | wall 9801\n",
      "2023-01-07 22:52:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-07 22:52:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 004:   0%|                                       | 0/6301 [00:00<?, ?it/s]2023-01-07 22:52:43 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-01-07 22:52:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:   1%| | 51/6301 [00:26<54:02,  1.93it/s, loss=5.773, ppl=54.67, wps=22023-01-07 22:53:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  19%|▏| 1222/6301 [10:31<43:31,  1.94it/s, loss=5.731, ppl=53.13, wps2023-01-07 23:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  38%|▍| 2393/6301 [20:36<33:44,  1.93it/s, loss=5.687, ppl=51.5, wps=2023-01-07 23:13:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  67%|▋| 4197/6301 [36:08<18:07,  1.93it/s, loss=5.705, ppl=52.16, wps2023-01-07 23:28:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  99%|▉| 6250/6301 [53:49<00:26,  1.93it/s, loss=5.666, ppl=50.79, wps2023-01-07 23:46:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 004: 100%|▉| 6300/6301 [54:15<00:00,  1.94it/s, loss=5.666, ppl=50.79, wps2023-01-07 23:46:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-07 23:46:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|              | 0/212 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   1%|      | 3/212 [00:00<00:07, 28.38it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   6%|▎    | 12/212 [00:00<00:03, 61.50it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  10%|▍    | 21/212 [00:00<00:02, 72.13it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  14%|▋    | 30/212 [00:00<00:02, 77.20it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  18%|▉    | 39/212 [00:00<00:02, 79.96it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  23%|█▏   | 48/212 [00:00<00:02, 81.45it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  27%|█▎   | 57/212 [00:00<00:01, 82.43it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  31%|█▌   | 66/212 [00:00<00:01, 83.09it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  35%|█▊   | 75/212 [00:00<00:01, 83.61it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  40%|█▉   | 84/212 [00:01<00:01, 83.95it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  44%|██▏  | 93/212 [00:01<00:01, 84.12it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  48%|█▉  | 102/212 [00:01<00:01, 84.15it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  52%|██  | 111/212 [00:01<00:01, 84.20it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  57%|██▎ | 120/212 [00:01<00:01, 84.30it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  61%|██▍ | 129/212 [00:01<00:00, 84.31it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  65%|██▌ | 138/212 [00:01<00:00, 84.37it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  69%|██▊ | 147/212 [00:01<00:00, 84.47it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  74%|██▉ | 156/212 [00:01<00:00, 84.49it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  78%|███ | 165/212 [00:02<00:00, 84.56it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  82%|███▎| 174/212 [00:02<00:00, 84.48it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  86%|███▍| 183/212 [00:02<00:00, 84.37it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  91%|███▌| 192/212 [00:02<00:00, 84.41it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  95%|███▊| 201/212 [00:02<00:00, 84.92it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|███▉| 211/212 [00:02<00:00, 87.42it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-07 23:47:02 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.407 | ppl 42.43 | wps 86660.3 | wpb 1021.8 | bsz 2 | num_updates 25180 | best_loss 5.407\n",
      "2023-01-07 23:47:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 25180 updates\n",
      "2023-01-07 23:47:02 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint4.pt\n",
      "2023-01-07 23:47:04 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint4.pt\n",
      "2023-01-07 23:47:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103/checkpoint4.pt (epoch 4 @ 25180 updates, score 5.407) (writing took 4.4466103330487385 seconds)\n",
      "2023-01-07 23:47:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-01-07 23:47:06 | INFO | train | epoch 004 | loss 5.689 | ppl 51.58 | wps 31610 | ups 1.93 | wpb 16382.5 | bsz 32 | num_updates 25180 | lr 0.000199284 | gnorm 0.816 | loss_scale 32 | train_wall 3194 | gb_free 18.1 | wall 13064\n",
      "2023-01-07 23:47:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-07 23:47:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n",
      "epoch 005:   0%|                                       | 0/6301 [00:00<?, ?it/s]2023-01-07 23:47:06 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-01-07 23:47:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  15%|▏| 975/6301 [08:23<45:59,  1.93it/s, loss=5.584, ppl=47.98, wps=2023-01-07 23:55:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 005:  32%|▎| 2000/6301 [17:13<37:01,  1.94it/s, loss=5.619, ppl=49.14, wps2023-01-08 00:04:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 005:  45%|▍| 2824/6301 [24:18<29:45,  1.95it/s, loss=5.646, ppl=50.06, wps2023-01-08 00:11:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005:  72%|▋| 4538/6301 [39:01<15:12,  1.93it/s, loss=5.567, ppl=47.4, wps=2023-01-08 00:26:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005: 100%|▉| 6300/6301 [54:11<00:00,  1.93it/s, loss=5.578, ppl=47.76, wps2023-01-08 00:41:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 00:41:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|              | 0/212 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   4%|▏     | 8/212 [00:00<00:02, 74.13it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▍    | 17/212 [00:00<00:02, 81.84it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  12%|▌    | 26/212 [00:00<00:02, 84.31it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  17%|▊    | 35/212 [00:00<00:02, 84.97it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  21%|█    | 44/212 [00:00<00:01, 84.89it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  25%|█▎   | 53/212 [00:00<00:01, 84.97it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  29%|█▍   | 62/212 [00:00<00:01, 84.99it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  33%|█▋   | 71/212 [00:00<00:01, 85.03it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  38%|█▉   | 80/212 [00:00<00:01, 84.91it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  42%|██   | 89/212 [00:01<00:01, 84.79it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  46%|██▎  | 98/212 [00:01<00:01, 84.75it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|██  | 107/212 [00:01<00:01, 84.78it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  55%|██▏ | 116/212 [00:01<00:01, 84.82it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  59%|██▎ | 125/212 [00:01<00:01, 84.88it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  63%|██▌ | 134/212 [00:01<00:00, 84.92it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  67%|██▋ | 143/212 [00:01<00:00, 85.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  72%|██▊ | 152/212 [00:01<00:00, 85.04it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  76%|███ | 161/212 [00:01<00:00, 85.10it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  80%|███▏| 170/212 [00:02<00:00, 85.13it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  84%|███▍| 179/212 [00:02<00:00, 85.08it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 005 | valid on 'valid' subset:  89%|███▌| 188/212 [00:02<00:00, 84.98it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  93%|███▋| 197/212 [00:02<00:00, 85.06it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  98%|███▉| 207/212 [00:02<00:00, 87.05it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 00:41:21 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.338 | ppl 40.46 | wps 87427.8 | wpb 1021.8 | bsz 2 | num_updates 31477 | best_loss 5.338\n",
      "2023-01-08 00:41:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 31477 updates\n",
      "2023-01-08 00:41:21 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint5.pt\n",
      "2023-01-08 00:41:23 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint5.pt\n",
      "2023-01-08 00:41:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103/checkpoint5.pt (epoch 5 @ 31477 updates, score 5.338) (writing took 4.4633170230081305 seconds)\n",
      "2023-01-08 00:41:25 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-01-08 00:41:25 | INFO | train | epoch 005 | loss 5.6 | ppl 48.52 | wps 31652.3 | ups 1.93 | wpb 16382.5 | bsz 32 | num_updates 31477 | lr 0.000178239 | gnorm 0.866 | loss_scale 32 | train_wall 3190 | gb_free 18.1 | wall 16323\n",
      "2023-01-08 00:41:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 00:41:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n",
      "epoch 006:   0%|                                       | 0/6301 [00:00<?, ?it/s]2023-01-08 00:41:25 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-01-08 00:41:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:   2%| | 97/6301 [00:50<53:27,  1.93it/s, loss=5.556, ppl=47.06, wps=22023-01-08 00:42:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006:  19%|▏| 1213/6301 [10:27<43:47,  1.94it/s, loss=5.533, ppl=46.29, wps2023-01-08 00:51:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006:  44%|▍| 2785/6301 [23:57<30:07,  1.95it/s, loss=5.524, ppl=46, wps=312023-01-08 01:05:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006:  70%|▋| 4405/6301 [37:51<16:14,  1.95it/s, loss=5.53, ppl=46.2, wps=32023-01-08 01:19:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006:  89%|▉| 5587/6301 [48:00<06:09,  1.93it/s, loss=5.551, ppl=46.87, wps2023-01-08 01:29:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006: 100%|▉| 6300/6301 [54:08<00:00,  1.95it/s, loss=5.524, ppl=46.03, wps2023-01-08 01:35:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 01:35:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|              | 0/212 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   3%|▏     | 7/212 [00:00<00:03, 65.45it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   8%|▍    | 16/212 [00:00<00:02, 76.20it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  12%|▌    | 25/212 [00:00<00:02, 79.78it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  16%|▊    | 34/212 [00:00<00:02, 81.70it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  20%|█    | 43/212 [00:00<00:02, 82.27it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  25%|█▏   | 52/212 [00:00<00:01, 82.91it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  29%|█▍   | 61/212 [00:00<00:01, 83.38it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  33%|█▋   | 70/212 [00:00<00:01, 83.69it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  37%|█▊   | 79/212 [00:00<00:01, 83.85it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  42%|██   | 88/212 [00:01<00:01, 83.82it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  46%|██▎  | 97/212 [00:01<00:01, 83.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|██  | 106/212 [00:01<00:01, 83.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  54%|██▏ | 115/212 [00:01<00:01, 83.44it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  58%|██▎ | 124/212 [00:01<00:01, 83.60it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  63%|██▌ | 133/212 [00:01<00:00, 83.61it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  67%|██▋ | 142/212 [00:01<00:00, 83.79it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  71%|██▊ | 151/212 [00:01<00:00, 83.88it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|███ | 160/212 [00:01<00:00, 83.90it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  80%|███▏| 169/212 [00:02<00:00, 83.91it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  84%|███▎| 178/212 [00:02<00:00, 83.93it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  88%|███▌| 187/212 [00:02<00:00, 84.02it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|███▋| 196/212 [00:02<00:00, 84.11it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  97%|███▊| 205/212 [00:02<00:00, 85.72it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 01:35:37 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.298 | ppl 39.35 | wps 85898.4 | wpb 1021.8 | bsz 2 | num_updates 37773 | best_loss 5.298\n",
      "2023-01-08 01:35:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 37773 updates\n",
      "2023-01-08 01:35:37 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint6.pt\n",
      "2023-01-08 01:35:39 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint6.pt\n",
      "2023-01-08 01:35:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103/checkpoint6.pt (epoch 6 @ 37773 updates, score 5.298) (writing took 4.397800639970228 seconds)\n",
      "2023-01-08 01:35:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-01-08 01:35:41 | INFO | train | epoch 006 | loss 5.537 | ppl 46.42 | wps 31679.5 | ups 1.93 | wpb 16382.5 | bsz 32 | num_updates 37773 | lr 0.000162708 | gnorm 0.906 | loss_scale 16 | train_wall 3186 | gb_free 18.1 | wall 19579\n",
      "2023-01-08 01:35:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 01:35:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n",
      "epoch 007:   0%|                                       | 0/6301 [00:00<?, ?it/s]2023-01-08 01:35:41 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-01-08 01:35:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:  14%|▏| 897/6301 [07:42<46:33,  1.93it/s, loss=5.485, ppl=44.78, wps=2023-01-08 01:43:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007:  32%|▎| 2000/6301 [17:12<36:55,  1.94it/s, loss=5.481, ppl=44.65, wps2023-01-08 01:52:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007:  51%|▌| 3187/6301 [27:24<27:25,  1.89it/s, loss=5.49, ppl=44.94, wps=2023-01-08 02:03:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007:  68%|▋| 4254/6301 [36:36<17:39,  1.93it/s, loss=5.495, ppl=45.11, wps2023-01-08 02:12:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007:  74%|▋| 4690/6301 [40:20<13:50,  1.94it/s, loss=5.52, ppl=45.88, wps=2023-01-08 02:16:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 007: 100%|▉| 6300/6301 [54:12<00:00,  1.94it/s, loss=5.501, ppl=45.28, wps2023-01-08 02:29:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 02:29:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 007 | valid on 'valid' subset:   0%|              | 0/212 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   3%|▏     | 7/212 [00:00<00:03, 68.10it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   8%|▍    | 16/212 [00:00<00:02, 77.63it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  12%|▌    | 25/212 [00:00<00:02, 80.84it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  16%|▊    | 34/212 [00:00<00:02, 82.37it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  20%|█    | 43/212 [00:00<00:02, 83.24it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  25%|█▏   | 52/212 [00:00<00:01, 83.70it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  29%|█▍   | 61/212 [00:00<00:01, 83.99it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  33%|█▋   | 70/212 [00:00<00:01, 83.78it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  37%|█▊   | 79/212 [00:00<00:01, 83.91it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  42%|██   | 88/212 [00:01<00:01, 83.93it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  46%|██▎  | 97/212 [00:01<00:01, 83.88it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|██  | 106/212 [00:01<00:01, 83.95it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  54%|██▏ | 115/212 [00:01<00:01, 84.00it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  58%|██▎ | 124/212 [00:01<00:01, 84.09it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  63%|██▌ | 133/212 [00:01<00:00, 83.65it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  67%|██▋ | 142/212 [00:01<00:00, 82.98it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  71%|██▊ | 151/212 [00:01<00:00, 83.39it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  75%|███ | 160/212 [00:01<00:00, 83.69it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  80%|███▏| 169/212 [00:02<00:00, 83.97it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  84%|███▎| 178/212 [00:02<00:00, 84.18it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  88%|███▌| 187/212 [00:02<00:00, 84.30it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  92%|███▋| 196/212 [00:02<00:00, 84.37it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  97%|███▊| 205/212 [00:02<00:00, 85.95it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 02:29:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.261 | ppl 38.35 | wps 86183.4 | wpb 1021.8 | bsz 2 | num_updates 44069 | best_loss 5.261\n",
      "2023-01-08 02:29:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 44069 updates\n",
      "2023-01-08 02:29:56 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint7.pt\n",
      "2023-01-08 02:29:58 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103/checkpoint7.pt\n",
      "2023-01-08 02:30:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103/checkpoint7.pt (epoch 7 @ 44069 updates, score 5.261) (writing took 4.669174724956974 seconds)\n",
      "2023-01-08 02:30:01 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-01-08 02:30:01 | INFO | train | epoch 007 | loss 5.493 | ppl 45.03 | wps 31641.3 | ups 1.93 | wpb 16382.5 | bsz 32 | num_updates 44069 | lr 0.000150638 | gnorm 0.951 | loss_scale 16 | train_wall 3191 | gb_free 18.1 | wall 22838\n",
      "2023-01-08 02:30:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 02:30:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 6301\n",
      "epoch 008:   0%|                                       | 0/6301 [00:00<?, ?it/s]2023-01-08 02:30:01 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-01-08 02:30:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  14%|▏| 886/6301 [07:37<46:17,  1.95it/s, loss=5.446, ppl=43.61, wps=2023-01-08 02:37:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 008:  28%|▎| 1775/6301 [15:15<38:51,  1.94it/s, loss=5.447, ppl=43.63, wps2023-01-08 02:45:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 008:  45%|▍| 2820/6301 [24:13<29:57,  1.94it/s, loss=5.457, ppl=43.92, wps^C\n",
      "Exception in thread Thread-10:                                                  \n",
      "Traceback (most recent call last):\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/threading.py\", line 870, in run\n",
      "Traceback (most recent call last):\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 49, in _pin_memory_loop\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 574, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "    do_one_step()\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 26, in do_one_step\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 205, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 331, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/trainer.py\", line 843, in train_step\n",
      "    loss, sample_size_i, logging_output = self.task.train_step(\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/tasks/fairseq_task.py\", line 531, in train_step\n",
      "    loss, sample_size, logging_output = criterion(model, sample)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/criterions/cross_entropy.py\", line 35, in forward\n",
      "    net_output = model(**sample[\"net_input\"])\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 305, in rebuild_storage_fd\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/fairseq_model.py\", line 506, in forward\n",
      "    return self.decoder(src_tokens, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
      "    x, extra = self.extract_features(\n",
      "    fd = df.detach()\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
      "    return self.extract_features_scriptable(\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 340, in extract_features_scriptable\n",
      "    x, layer_attn, _ = layer(\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/modules/transformer_layer.py\", line 451, in forward\n",
      "    x, attn = self.self_attn(\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/modules/multihead_attention.py\", line 539, in forward\n",
      "    return F.multi_head_attention_forward(\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/functional.py\", line 5055, in multi_head_attention_forward\n",
      "    q, k, v = _in_projection(query, key, value, q_proj_weight, k_proj_weight, v_proj_weight, b_q, b_k, b_v)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/functional.py\", line 4805, in _in_projection\n",
      "    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)\n",
      "KeyboardInterrupt\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/multiprocessing/connection.py\", line 502, in Client\n",
      "    c = SocketClient(address)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/multiprocessing/connection.py\", line 630, in SocketClient\n",
      "    s.connect(address)\n",
      "FileNotFoundError: [Errno 2] No such file or directory\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train --task language_modeling \\\n",
    "  data-bin/wikitext-103 \\\n",
    "  --save-dir checkpoints/transformer_wikitext-103 \\\n",
    "  --arch transformer_lm --share-decoder-input-output-embed \\\n",
    "  --dropout 0.1 \\\n",
    "  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\n",
    "  --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "  --tokens-per-sample 512 --sample-break-mode none \\\n",
    "  --max-tokens 1024 --update-freq 16 \\\n",
    "  --fp16 \\\n",
    "  --max-update 50000 \\\n",
    "  --wandb-project  NLP_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d327b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 02:54:16 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-01-08 02:54:18 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'NLP_test', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/transformer_wikitext-103-mt2048', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 02:54:18 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types\n",
      "2023-01-08 02:54:20 | INFO | fairseq_cli.train | TransformerLanguageModel(\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(267744, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=267744, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-01-08 02:54:20 | INFO | fairseq_cli.train | task: LanguageModelingTask\n",
      "2023-01-08 02:54:20 | INFO | fairseq_cli.train | model: TransformerLanguageModel\n",
      "2023-01-08 02:54:20 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
      "2023-01-08 02:54:20 | INFO | fairseq_cli.train | num. shared model params: 155,999,232 (num. trained: 155,999,232)\n",
      "2023-01-08 02:54:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-01-08 02:54:20 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103/valid\n",
      "2023-01-08 02:54:21 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-01-08 02:54:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 02:54:21 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 \n",
      "2023-01-08 02:54:21 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 02:54:21 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-01-08 02:54:21 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None\n",
      "2023-01-08 02:54:21 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint_last.pt\n",
      "2023-01-08 02:54:21 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/transformer_wikitext-103-mt2048/checkpoint_last.pt\n",
      "2023-01-08 02:54:21 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-01-08 02:54:21 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103/train\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 02:54:21 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 02:54:21 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 02:54:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001:   0%|                                       | 0/3151 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzh110\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/wandb/run-20230108_025423-1j447rcv\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtransformer_wikitext-103-mt2048\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test/runs/1j447rcv\u001b[0m\n",
      "2023-01-08 02:54:30 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-01-08 02:54:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-01-08 02:54:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                             | 1/3151 [00:10<9:10:41, 10.49s/it]2023-01-08 02:54:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   0%|                             | 2/3151 [00:12<4:45:34,  5.44s/it]2023-01-08 02:54:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   0%|                             | 3/3151 [00:14<3:18:50,  3.79s/it]2023-01-08 02:54:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001:  98%|▉| 3103/3151 [46:42<00:43,  1.11it/s, loss=7.022, ppl=129.94, wp2023-01-08 03:41:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001: 100%|▉| 3150/3151 [47:24<00:00,  1.11it/s, loss=6.968, ppl=125.17, wp2023-01-08 03:41:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 03:41:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 43.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:01, 47.87it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:01, 49.15it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  22%|█    | 23/106 [00:00<00:01, 49.63it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:00<00:01, 49.84it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 50.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:00<00:01, 50.20it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:00<00:01, 50.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:01<00:01, 50.12it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:01<00:00, 50.22it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:00, 50.23it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:01<00:00, 50.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:01<00:00, 50.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:01<00:00, 50.42it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  84%|████▏| 89/106 [00:01<00:00, 50.53it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  90%|████▍| 95/106 [00:01<00:00, 50.58it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  95%|███▊| 101/106 [00:02<00:00, 50.84it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 03:41:49 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 6.668 | ppl 101.68 | wps 102756 | wpb 2043.6 | bsz 4 | num_updates 3146\n",
      "2023-01-08 03:41:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3146 updates\n",
      "2023-01-08 03:41:49 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint1.pt\n",
      "2023-01-08 03:41:51 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint1.pt\n",
      "2023-01-08 03:41:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint1.pt (epoch 1 @ 3146 updates, score 6.668) (writing took 3.1226950719719753 seconds)\n",
      "2023-01-08 03:41:52 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-01-08 03:41:52 | INFO | train | epoch 001 | loss 8.774 | ppl 437.81 | wps 36378 | ups 1.11 | wpb 32759.4 | bsz 64 | num_updates 3146 | lr 0.000393271 | gnorm 0.933 | loss_scale 32 | train_wall 2803 | gb_free 15.4 | wall 2851\n",
      "2023-01-08 03:41:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 03:41:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 002:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 03:41:52 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-01-08 03:41:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  31%|▎| 983/3151 [14:43<32:27,  1.11it/s, loss=6.553, ppl=93.89, wps=2023-01-08 03:56:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 002:  37%|▎| 1156/3151 [17:18<29:52,  1.11it/s, loss=6.472, ppl=88.77, wps2023-01-08 03:59:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 002:  85%|▊| 2684/3151 [40:10<06:59,  1.11it/s, loss=6.117, ppl=69.38, wps2023-01-08 04:22:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 002: 100%|▉| 3150/3151 [47:09<00:00,  1.11it/s, loss=6.061, ppl=66.78, wps2023-01-08 04:29:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 04:29:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 45.54it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:01, 48.92it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:01, 49.96it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  22%|█    | 23/106 [00:00<00:01, 50.45it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:00<00:01, 50.62it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 50.76it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:00<00:01, 50.86it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:00<00:01, 50.66it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:01<00:01, 50.73it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:01<00:00, 50.76it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:00, 50.80it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:01<00:00, 50.89it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:01<00:00, 50.96it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:01<00:00, 51.01it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  84%|████▏| 89/106 [00:01<00:00, 51.04it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  90%|████▍| 95/106 [00:01<00:00, 51.08it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  95%|███▊| 101/106 [00:01<00:00, 51.25it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 04:29:03 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 5.768 | ppl 54.49 | wps 103819 | wpb 2043.6 | bsz 4 | num_updates 6294 | best_loss 5.768\n",
      "2023-01-08 04:29:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 6294 updates\n",
      "2023-01-08 04:29:03 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint2.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 04:29:05 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint2.pt\n",
      "2023-01-08 04:29:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint2.pt (epoch 2 @ 6294 updates, score 5.768) (writing took 4.514163293060847 seconds)\n",
      "2023-01-08 04:29:08 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-01-08 04:29:08 | INFO | train | epoch 002 | loss 6.381 | ppl 83.33 | wps 36363.9 | ups 1.11 | wpb 32759.4 | bsz 64 | num_updates 6294 | lr 0.000398599 | gnorm 0.643 | loss_scale 16 | train_wall 2795 | gb_free 15.4 | wall 5687\n",
      "2023-01-08 04:29:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 04:29:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 003:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 04:29:08 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-01-08 04:29:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  20%|▏| 645/3151 [09:39<37:31,  1.11it/s, loss=5.914, ppl=60.28, wps=2023-01-08 04:38:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 003:  53%|▌| 1675/3151 [25:05<22:06,  1.11it/s, loss=5.836, ppl=57.11, wps2023-01-08 04:54:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 003:  88%|▉| 2769/3151 [41:27<05:43,  1.11it/s, loss=5.742, ppl=53.53, wps2023-01-08 05:10:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 003: 100%|▉| 3150/3151 [47:09<00:00,  1.12it/s, loss=5.749, ppl=53.77, wps2023-01-08 05:16:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 05:16:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   4%|▏     | 4/106 [00:00<00:02, 35.71it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   9%|▍    | 10/106 [00:00<00:02, 44.88it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|▊    | 16/106 [00:00<00:01, 47.51it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  21%|█    | 22/106 [00:00<00:01, 48.79it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  26%|█▎   | 28/106 [00:00<00:01, 49.50it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  32%|█▌   | 34/106 [00:00<00:01, 49.82it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|█▉   | 40/106 [00:00<00:01, 50.15it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  43%|██▏  | 46/106 [00:00<00:01, 50.15it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  49%|██▍  | 52/106 [00:01<00:01, 50.07it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  55%|██▋  | 58/106 [00:01<00:00, 50.13it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  60%|███  | 64/106 [00:01<00:00, 50.19it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  66%|███▎ | 70/106 [00:01<00:00, 50.28it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  72%|███▌ | 76/106 [00:01<00:00, 50.37it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|███▊ | 82/106 [00:01<00:00, 50.51it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  83%|████▏| 88/106 [00:01<00:00, 50.57it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  89%|████▍| 94/106 [00:01<00:00, 50.63it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  94%|███▊| 100/106 [00:02<00:00, 50.91it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████| 106/106 [00:02<00:00, 51.10it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 05:16:20 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.496 | ppl 45.12 | wps 102951 | wpb 2043.6 | bsz 4 | num_updates 9442 | best_loss 5.496\n",
      "2023-01-08 05:16:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 9442 updates\n",
      "2023-01-08 05:16:20 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint3.pt\n",
      "2023-01-08 05:16:22 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint3.pt\n",
      "2023-01-08 05:16:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint3.pt (epoch 3 @ 9442 updates, score 5.496) (writing took 4.47389892488718 seconds)\n",
      "2023-01-08 05:16:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-01-08 05:16:24 | INFO | train | epoch 003 | loss 5.839 | ppl 57.25 | wps 36354.1 | ups 1.11 | wpb 32759.4 | bsz 64 | num_updates 9442 | lr 0.000325438 | gnorm 0.593 | loss_scale 16 | train_wall 2796 | gb_free 15.4 | wall 8524\n",
      "2023-01-08 05:16:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 05:16:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 004:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 05:16:25 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-01-08 05:16:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:  21%|▏| 671/3151 [10:02<37:07,  1.11it/s, loss=5.64, ppl=49.87, wps=32023-01-08 05:26:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  54%|▌| 1706/3151 [25:31<21:38,  1.11it/s, loss=5.599, ppl=48.48, wps2023-01-08 05:41:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004:  87%|▊| 2735/3151 [40:55<06:12,  1.12it/s, loss=5.602, ppl=48.57, wps2023-01-08 05:57:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 004: 100%|▉| 3150/3151 [47:07<00:00,  1.12it/s, loss=5.576, ppl=47.71, wps2023-01-08 06:03:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 06:03:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 43.36it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:02, 47.44it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:01, 48.79it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  22%|█    | 23/106 [00:00<00:01, 49.46it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:00<00:01, 49.79it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 50.00it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:00<00:01, 50.15it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:00<00:01, 50.21it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:01<00:01, 50.21it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:01<00:00, 50.02it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:00, 50.26it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:01<00:00, 50.47it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:01<00:00, 50.55it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:01<00:00, 50.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  84%|████▏| 89/106 [00:01<00:00, 50.74it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  90%|████▍| 95/106 [00:01<00:00, 50.80it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  95%|███▊| 101/106 [00:02<00:00, 50.98it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 06:03:35 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 5.355 | ppl 40.92 | wps 102833 | wpb 2043.6 | bsz 4 | num_updates 12590 | best_loss 5.355\n",
      "2023-01-08 06:03:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 12590 updates\n",
      "2023-01-08 06:03:35 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint4.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 06:03:37 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint4.pt\n",
      "2023-01-08 06:03:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint4.pt (epoch 4 @ 12590 updates, score 5.355) (writing took 4.525860294001177 seconds)\n",
      "2023-01-08 06:03:39 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-01-08 06:03:39 | INFO | train | epoch 004 | loss 5.613 | ppl 48.95 | wps 36380.8 | ups 1.11 | wpb 32759.4 | bsz 64 | num_updates 12590 | lr 0.00028183 | gnorm 0.603 | loss_scale 16 | train_wall 2794 | gb_free 15.4 | wall 11358\n",
      "2023-01-08 06:03:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 06:03:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 005:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 06:03:39 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-01-08 06:03:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:  28%|▎| 885/3151 [13:15<33:54,  1.11it/s, loss=5.478, ppl=44.56, wps=2023-01-08 06:16:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005:  62%|▌| 1938/3151 [29:00<18:09,  1.11it/s, loss=5.477, ppl=44.54, wps2023-01-08 06:32:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005:  97%|▉| 3060/3151 [45:48<01:21,  1.11it/s, loss=5.482, ppl=44.69, wps2023-01-08 06:49:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 005: 100%|▉| 3150/3151 [47:09<00:00,  1.11it/s, loss=5.486, ppl=44.83, wps2023-01-08 06:50:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 06:50:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 43.33it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:01, 47.74it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:01, 49.15it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  22%|█    | 23/106 [00:00<00:01, 49.64it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:00<00:01, 49.99it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 50.11it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:00<00:01, 50.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:00<00:01, 50.31it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:01<00:01, 50.20it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:01<00:00, 50.21it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:00, 50.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:01<00:00, 50.39it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:01<00:00, 50.41it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:01<00:00, 50.46it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  84%|████▏| 89/106 [00:01<00:00, 50.45it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  90%|████▍| 95/106 [00:01<00:00, 50.45it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  95%|███▊| 101/106 [00:02<00:00, 50.80it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 06:50:51 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 5.27 | ppl 38.6 | wps 102762 | wpb 2043.6 | bsz 4 | num_updates 15738 | best_loss 5.27\n",
      "2023-01-08 06:50:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 15738 updates\n",
      "2023-01-08 06:50:51 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint5.pt\n",
      "2023-01-08 06:50:53 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint5.pt\n",
      "2023-01-08 06:50:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint5.pt (epoch 5 @ 15738 updates, score 5.27) (writing took 4.369467996060848 seconds)\n",
      "2023-01-08 06:50:55 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-01-08 06:50:55 | INFO | train | epoch 005 | loss 5.481 | ppl 44.66 | wps 36360.7 | ups 1.11 | wpb 32759.4 | bsz 64 | num_updates 15738 | lr 0.000252072 | gnorm 0.62 | loss_scale 16 | train_wall 2795 | gb_free 15.4 | wall 14194\n",
      "2023-01-08 06:50:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 06:50:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 006:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 06:50:55 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-01-08 06:50:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 006:  40%|▍| 1249/3151 [18:41<28:27,  1.11it/s, loss=5.403, ppl=42.31, wps2023-01-08 07:09:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006:  77%|▊| 2425/3151 [36:16<10:51,  1.11it/s, loss=5.4, ppl=42.21, wps=32023-01-08 07:27:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 006: 100%|▉| 3150/3151 [47:07<00:00,  1.11it/s, loss=5.394, ppl=42.06, wps2023-01-08 07:38:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 07:38:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 44.04it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:01, 48.12it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:01, 49.31it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  22%|█    | 23/106 [00:00<00:01, 49.89it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:00<00:01, 50.17it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 50.32it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:00<00:01, 50.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:00<00:01, 50.36it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:01<00:01, 50.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:01<00:00, 50.25it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:00, 50.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:01<00:00, 50.28it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:01<00:00, 50.23it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:01<00:00, 50.33it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  84%|████▏| 89/106 [00:01<00:00, 50.26it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  90%|████▍| 95/106 [00:01<00:00, 50.25it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  95%|███▊| 101/106 [00:02<00:00, 50.61it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 07:38:05 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.21 | ppl 37.02 | wps 102577 | wpb 2043.6 | bsz 4 | num_updates 18887 | best_loss 5.21\n",
      "2023-01-08 07:38:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 18887 updates\n",
      "2023-01-08 07:38:05 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint6.pt\n",
      "2023-01-08 07:38:07 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint6.pt\n",
      "2023-01-08 07:38:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint6.pt (epoch 6 @ 18887 updates, score 5.21) (writing took 4.391447634901851 seconds)\n",
      "2023-01-08 07:38:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-01-08 07:38:10 | INFO | train | epoch 006 | loss 5.391 | ppl 41.96 | wps 36398.6 | ups 1.11 | wpb 32759.4 | bsz 64 | num_updates 18887 | lr 0.000230101 | gnorm 0.644 | loss_scale 16 | train_wall 2794 | gb_free 15.4 | wall 17029\n",
      "2023-01-08 07:38:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 07:38:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 007:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 07:38:10 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-01-08 07:38:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:  14%|▏| 426/3151 [06:22<40:46,  1.11it/s, loss=5.294, ppl=39.25, wps=2023-01-08 07:44:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007:  46%|▍| 1460/3151 [21:50<25:18,  1.11it/s, loss=5.328, ppl=40.17, wps2023-01-08 08:00:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007:  87%|▊| 2740/3151 [40:59<06:09,  1.11it/s, loss=5.337, ppl=40.42, wps2023-01-08 08:19:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 007: 100%|▉| 3150/3151 [47:07<00:00,  1.11it/s, loss=5.338, ppl=40.44, wps2023-01-08 08:25:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 08:25:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 007 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 43.90it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:01, 47.96it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:01, 49.10it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  22%|█    | 23/106 [00:00<00:01, 49.74it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:00<00:01, 50.07it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 50.34it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:00<00:01, 50.40it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:00<00:01, 50.45it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:01<00:01, 50.36it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:01<00:00, 50.33it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:00, 50.39it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:01<00:00, 50.50it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:01<00:00, 50.59it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:01<00:00, 50.69it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  84%|████▏| 89/106 [00:01<00:00, 50.69it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  90%|████▍| 95/106 [00:01<00:00, 50.74it/s]\u001b[A\n",
      "epoch 007 | valid on 'valid' subset:  95%|███▊| 101/106 [00:02<00:00, 51.00it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 08:25:19 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 5.168 | ppl 35.95 | wps 103033 | wpb 2043.6 | bsz 4 | num_updates 22035 | best_loss 5.168\n",
      "2023-01-08 08:25:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 22035 updates\n",
      "2023-01-08 08:25:19 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint7.pt\n",
      "2023-01-08 08:25:21 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048/checkpoint7.pt\n",
      "2023-01-08 08:25:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048/checkpoint7.pt (epoch 7 @ 22035 updates, score 5.168) (writing took 4.470314293168485 seconds)\n",
      "2023-01-08 08:25:24 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
      "2023-01-08 08:25:24 | INFO | train | epoch 007 | loss 5.326 | ppl 40.11 | wps 36388.6 | ups 1.11 | wpb 32759.4 | bsz 64 | num_updates 22035 | lr 0.000213031 | gnorm 0.658 | loss_scale 16 | train_wall 2793 | gb_free 15.4 | wall 19863\n",
      "2023-01-08 08:25:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 08:25:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 008:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 08:25:24 | INFO | fairseq.trainer | begin training epoch 8\n",
      "2023-01-08 08:25:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 008:  50%|▌| 1581/3151 [23:39<23:30,  1.11it/s, loss=5.276, ppl=38.75, wps2023-01-08 08:49:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 008:  84%|▊| 2658/3151 [39:46<07:23,  1.11it/s, loss=5.285, ppl=39, wps=362023-01-08 09:05:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 008:  93%|▉| 2917/3151 [43:55<06:10,  1.58s/it, loss=5.284, ppl=38.97, wps"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train --task language_modeling \\\n",
    "  data-bin/wikitext-103 \\\n",
    "  --save-dir checkpoints/transformer_wikitext-103-mt2048 \\\n",
    "  --arch transformer_lm --share-decoder-input-output-embed \\\n",
    "  --dropout 0.1 \\\n",
    "  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\n",
    "  --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \\\n",
    "  --tokens-per-sample 512 --sample-break-mode none \\\n",
    "  --max-tokens 2048 --update-freq 16 \\\n",
    "  --fp16 \\\n",
    "  --max-update 50000 \\\n",
    "  --wandb-project  NLP_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8db8cf79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 13:27:22 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2023-01-08 13:27:24 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': 'NLP_test', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 2048, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 2048, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [16], 'lr': [5e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/transformer_wikitext-103-mt2048-lr05', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'transformer_lm', 'activation_fn': relu, 'dropout': 0.1, 'attention_dropout': 0.0, 'activation_dropout': 0.0, 'relu_dropout': 0.0, 'decoder_embed_dim': 512, 'decoder_output_dim': 512, 'decoder_input_dim': 512, 'decoder_ffn_embed_dim': 2048, 'decoder_layers': 6, 'decoder_attention_heads': 8, 'decoder_normalize_before': False, 'no_decoder_final_norm': False, 'adaptive_softmax_cutoff': None, 'adaptive_softmax_dropout': 0.0, 'adaptive_softmax_factor': 4.0, 'no_token_positional_embeddings': False, 'share_decoder_input_output_embed': True, 'character_embeddings': False, 'character_filters': '[(1, 64), (2, 128), (3, 192), (4, 256), (5, 256), (6, 256), (7, 256)]', 'character_embedding_dim': 4, 'char_embedder_highway_layers': 2, 'adaptive_input': False, 'adaptive_input_factor': 4.0, 'adaptive_input_cutoff': None, 'tie_adaptive_weights': False, 'tie_adaptive_proj': False, 'decoder_learned_pos': False, 'layernorm_embedding': False, 'no_scale_embedding': False, 'checkpoint_activations': False, 'offload_activations': False, 'decoder_layerdrop': 0.0, 'decoder_layers_to_keep': None, 'quant_noise_pq': 0.0, 'quant_noise_pq_block_size': 8, 'quant_noise_scalar': 0.0, 'min_params_to_wrap': 100000000, 'base_layers': 0, 'base_sublayers': 1, 'base_shuffle': 1, 'scale_fc': False, 'scale_attn': False, 'scale_heads': False, 'scale_resids': False, 'decoder_xformers_att_config': None, 'add_bos_token': False, 'tokens_per_sample': 512, 'max_target_positions': None, 'tpu': False}, 'task': {'_name': 'language_modeling', 'data': 'data-bin/wikitext-103', 'sample_break_mode': none, 'tokens_per_sample': 512, 'output_dictionary_size': -1, 'self_target': False, 'future_target': False, 'past_target': False, 'add_bos_token': False, 'max_target_positions': None, 'shorten_method': none, 'shorten_data_split_list': '', 'pad_to_fixed_length': False, 'pad_to_fixed_bsz': False, 'seed': 1, 'batch_size': None, 'batch_size_valid': None, 'dataset_impl': None, 'data_buffer_size': 10, 'tpu': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [5e-05]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 2000, 'warmup_init_lr': 1e-07, 'lr': [5e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 13:27:24 | INFO | fairseq.tasks.language_modeling | dictionary: 267744 types\n",
      "2023-01-08 13:27:27 | INFO | fairseq_cli.train | TransformerLanguageModel(\n",
      "  (decoder): TransformerDecoder(\n",
      "    (dropout_module): FairseqDropout()\n",
      "    (embed_tokens): Embedding(267744, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayerBase(\n",
      "        (dropout_module): FairseqDropout()\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (dropout_module): FairseqDropout()\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (activation_dropout_module): FairseqDropout()\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (output_projection): Linear(in_features=512, out_features=267744, bias=False)\n",
      "  )\n",
      ")\n",
      "2023-01-08 13:27:27 | INFO | fairseq_cli.train | task: LanguageModelingTask\n",
      "2023-01-08 13:27:27 | INFO | fairseq_cli.train | model: TransformerLanguageModel\n",
      "2023-01-08 13:27:27 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
      "2023-01-08 13:27:27 | INFO | fairseq_cli.train | num. shared model params: 155,999,232 (num. trained: 155,999,232)\n",
      "2023-01-08 13:27:27 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
      "2023-01-08 13:27:27 | INFO | fairseq.data.data_utils | loaded 3,760 examples from: data-bin/wikitext-103/valid\n",
      "2023-01-08 13:27:28 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
      "2023-01-08 13:27:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 13:27:28 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 \n",
      "2023-01-08 13:27:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
      "2023-01-08 13:27:28 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
      "2023-01-08 13:27:28 | INFO | fairseq_cli.train | max tokens per device = 2048 and max sentences per device = None\n",
      "2023-01-08 13:27:28 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint_last.pt\n",
      "2023-01-08 13:27:28 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint_last.pt\n",
      "2023-01-08 13:27:28 | INFO | fairseq.trainer | loading train data for epoch 1\n",
      "2023-01-08 13:27:28 | INFO | fairseq.data.data_utils | loaded 1,801,350 examples from: data-bin/wikitext-103/train\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 13:27:28 | INFO | fairseq_cli.train | begin dry-run validation on \"valid\" subset\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
      "2023-01-08 13:27:28 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
      "2023-01-08 13:27:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 001:   0%|                                       | 0/3151 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzh110\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/wandb/run-20230108_132730-39js80z1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mtransformer_wikitext-103-mt2048-lr05\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/zzh110/NLP_test/runs/39js80z1\u001b[0m\n",
      "2023-01-08 13:27:36 | INFO | fairseq.trainer | begin training epoch 1\n",
      "2023-01-08 13:27:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "2023-01-08 13:27:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 001:   0%|                            | 1/3151 [00:15<13:12:32, 15.10s/it]2023-01-08 13:27:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
      "epoch 001:   0%|                             | 2/3151 [00:19<7:32:44,  8.63s/it]2023-01-08 13:27:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
      "epoch 001:   0%|                             | 3/3151 [00:22<5:36:44,  6.42s/it]2023-01-08 13:27:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
      "epoch 001: 100%|▉| 3150/3151 [1:43:04<00:01,  1.29s/it, loss=8.359, ppl=328.26, 2023-01-08 15:10:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 15:10:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 001 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 44.61it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:   9%|▍    | 10/106 [00:00<00:02, 47.45it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  14%|▋    | 15/106 [00:00<00:01, 48.43it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:01, 48.78it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  24%|█▏   | 25/106 [00:00<00:01, 48.65it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  28%|█▍   | 30/106 [00:00<00:01, 48.48it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 48.56it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  38%|█▉   | 40/106 [00:00<00:01, 48.91it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  42%|██   | 45/106 [00:00<00:01, 49.08it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  47%|██▎  | 50/106 [00:01<00:01, 49.13it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  52%|██▌  | 55/106 [00:01<00:01, 49.17it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  57%|██▊  | 60/106 [00:01<00:00, 48.64it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:01, 34.72it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  66%|███▎ | 70/106 [00:01<00:01, 28.31it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  70%|███▍ | 74/106 [00:01<00:01, 25.39it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:02<00:01, 23.95it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  75%|███▊ | 80/106 [00:02<00:01, 22.71it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:02<00:01, 21.95it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  81%|████ | 86/106 [00:02<00:00, 21.43it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  84%|████▏| 89/106 [00:02<00:00, 21.15it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  87%|████▎| 92/106 [00:02<00:00, 20.74it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  90%|████▍| 95/106 [00:02<00:00, 21.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  92%|████▌| 98/106 [00:03<00:00, 20.86it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  95%|███▊| 101/106 [00:03<00:00, 20.59it/s]\u001b[A\n",
      "epoch 001 | valid on 'valid' subset:  98%|███▉| 104/106 [00:03<00:00, 20.26it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 15:10:36 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.095 | ppl 273.38 | wps 61280.1 | wpb 2043.6 | bsz 4 | num_updates 3147\n",
      "2023-01-08 15:10:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 3147 updates\n",
      "2023-01-08 15:10:36 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint1.pt\n",
      "2023-01-08 15:10:39 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint1.pt\n",
      "2023-01-08 15:10:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint1.pt (epoch 1 @ 3147 updates, score 8.095) (writing took 3.6407796191051602 seconds)\n",
      "2023-01-08 15:10:40 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
      "2023-01-08 15:10:40 | INFO | train | epoch 001 | loss 10.249 | ppl 1216.66 | wps 16726.4 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 3147 | lr 3.98599e-05 | gnorm 1.025 | loss_scale 64 | train_wall 6149 | gb_free 15.4 | wall 6192\n",
      "2023-01-08 15:10:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 15:10:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 002:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 15:10:40 | INFO | fairseq.trainer | begin training epoch 2\n",
      "2023-01-08 15:10:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 002:  40%|▍| 1255/3151 [40:56<1:01:51,  1.96s/it, loss=8, ppl=255.98, wps=2023-01-08 15:51:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 002:  79%|▊| 2501/3151 [1:21:35<21:11,  1.96s/it, loss=7.774, ppl=218.95, 2023-01-08 16:32:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 002: 100%|▉| 3150/3151 [1:42:41<00:01,  1.31s/it, loss=7.7, ppl=207.94, wp2023-01-08 16:53:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 16:53:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 002 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 44.56it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:   9%|▍    | 10/106 [00:00<00:02, 46.68it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  14%|▋    | 15/106 [00:00<00:01, 47.72it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:01, 48.37it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  24%|█▏   | 25/106 [00:00<00:01, 48.42it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  28%|█▍   | 30/106 [00:00<00:01, 48.48it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:00<00:01, 48.78it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  38%|█▉   | 40/106 [00:00<00:01, 49.07it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  42%|██   | 45/106 [00:00<00:01, 48.89it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  47%|██▎  | 50/106 [00:01<00:01, 48.82it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  52%|██▌  | 55/106 [00:01<00:01, 48.97it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  57%|██▊  | 60/106 [00:01<00:00, 48.70it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  61%|███  | 65/106 [00:01<00:00, 48.60it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  66%|███▎ | 70/106 [00:01<00:00, 48.74it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  71%|███▌ | 75/106 [00:01<00:00, 48.89it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  75%|███▊ | 80/106 [00:01<00:00, 48.79it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  80%|████ | 85/106 [00:01<00:00, 48.87it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  85%|████▏| 90/106 [00:01<00:00, 49.12it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  90%|████▍| 95/106 [00:01<00:00, 49.09it/s]\u001b[A\n",
      "epoch 002 | valid on 'valid' subset:  95%|███▊| 101/106 [00:02<00:00, 49.55it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \u001b[A2023-01-08 16:53:24 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 7.403 | ppl 169.21 | wps 99826.4 | wpb 2043.6 | bsz 4 | num_updates 6296 | best_loss 7.403\n",
      "2023-01-08 16:53:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 6296 updates\n",
      "2023-01-08 16:53:24 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint2.pt\n",
      "2023-01-08 16:53:26 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint2.pt\n",
      "2023-01-08 16:53:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint2.pt (epoch 2 @ 6296 updates, score 7.403) (writing took 6.535982402972877 seconds)\n",
      "2023-01-08 16:53:31 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
      "2023-01-08 16:53:31 | INFO | train | epoch 002 | loss 7.945 | ppl 246.44 | wps 16717.1 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 6296 | lr 2.81808e-05 | gnorm 0.849 | loss_scale 64 | train_wall 6134 | gb_free 15.4 | wall 12363\n",
      "2023-01-08 16:53:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 16:53:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 003:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 16:53:31 | INFO | fairseq.trainer | begin training epoch 3\n",
      "2023-01-08 16:53:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 003:  30%|▎| 961/3151 [31:18<1:11:16,  1.95s/it, loss=7.554, ppl=187.95, w2023-01-08 17:24:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 003:  67%|▋| 2104/3151 [1:08:30<34:04,  1.95s/it, loss=7.465, ppl=176.71, 2023-01-08 18:02:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 003: 100%|▉| 3150/3151 [1:42:28<00:01,  1.16s/it, loss=7.369, ppl=165.31, 2023-01-08 18:36:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 18:36:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 003 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   4%|▏     | 4/106 [00:00<00:02, 35.05it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:   9%|▍    | 10/106 [00:00<00:02, 44.39it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  15%|▊    | 16/106 [00:00<00:01, 47.26it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  21%|█    | 22/106 [00:00<00:01, 48.72it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  26%|█▎   | 28/106 [00:00<00:01, 49.32it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  32%|█▌   | 34/106 [00:00<00:01, 49.66it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  38%|█▉   | 40/106 [00:00<00:01, 50.03it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  43%|██▏  | 46/106 [00:00<00:01, 50.14it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  49%|██▍  | 52/106 [00:01<00:01, 50.02it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  55%|██▋  | 58/106 [00:01<00:00, 49.96it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  59%|██▉  | 63/106 [00:01<00:01, 39.49it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  64%|███▏ | 68/106 [00:01<00:01, 31.58it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  68%|███▍ | 72/106 [00:01<00:01, 28.09it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  72%|███▌ | 76/106 [00:02<00:01, 25.68it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  75%|███▋ | 79/106 [00:02<00:01, 24.19it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  77%|███▊ | 82/106 [00:02<00:01, 23.00it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  80%|████ | 85/106 [00:02<00:00, 22.54it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  83%|████▏| 88/106 [00:02<00:00, 21.77it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  86%|████▎| 91/106 [00:02<00:00, 21.34it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  89%|████▍| 94/106 [00:02<00:00, 21.33it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  92%|████▌| 97/106 [00:03<00:00, 22.37it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  94%|███▊| 100/106 [00:03<00:00, 21.80it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset:  97%|███▉| 103/106 [00:03<00:00, 21.41it/s]\u001b[A\n",
      "epoch 003 | valid on 'valid' subset: 100%|████| 106/106 [00:03<00:00, 21.43it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 18:36:03 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 7.084 | ppl 135.68 | wps 63026.3 | wpb 2043.6 | bsz 4 | num_updates 9445 | best_loss 7.084\n",
      "2023-01-08 18:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 9445 updates\n",
      "2023-01-08 18:36:03 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint3.pt\n",
      "2023-01-08 18:36:05 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint3.pt\n",
      "2023-01-08 18:36:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint3.pt (epoch 3 @ 9445 updates, score 7.084) (writing took 5.479401188902557 seconds)\n",
      "2023-01-08 18:36:09 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
      "2023-01-08 18:36:09 | INFO | train | epoch 003 | loss 7.498 | ppl 180.71 | wps 16752.3 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 9445 | lr 2.30083e-05 | gnorm 0.851 | loss_scale 128 | train_wall 6115 | gb_free 15.4 | wall 18521\n",
      "2023-01-08 18:36:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 18:36:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 004:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 18:36:09 | INFO | fairseq.trainer | begin training epoch 4\n",
      "2023-01-08 18:36:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 004:   0%|                            | 12/3151 [00:23<1:42:01,  1.95s/it]2023-01-08 18:36:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 004:  37%|▎| 1178/3151 [38:16<1:04:03,  1.95s/it, loss=7.266, ppl=153.91, 2023-01-08 19:14:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 004:  71%|▋| 2238/3151 [1:12:41<29:38,  1.95s/it, loss=7.215, ppl=148.59, 2023-01-08 19:48:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 004: 100%|▉| 3150/3151 [1:42:13<00:01,  1.09s/it, loss=7.169, ppl=143.87, 2023-01-08 20:18:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 20:18:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 004 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 43.36it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:01, 47.63it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:01, 48.90it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  21%|█    | 22/106 [00:00<00:02, 38.14it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  25%|█▎   | 27/106 [00:00<00:02, 29.47it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  29%|█▍   | 31/106 [00:00<00:02, 26.12it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  32%|█▌   | 34/106 [00:01<00:02, 24.49it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  35%|█▋   | 37/106 [00:01<00:02, 23.35it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  38%|█▉   | 40/106 [00:01<00:02, 22.56it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  41%|██   | 43/106 [00:01<00:02, 22.14it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  43%|██▏  | 46/106 [00:01<00:02, 21.68it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  46%|██▎  | 49/106 [00:01<00:02, 21.30it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 004 | valid on 'valid' subset:  49%|██▍  | 52/106 [00:02<00:02, 20.72it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  52%|██▌  | 55/106 [00:02<00:02, 21.90it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  55%|██▋  | 58/106 [00:02<00:02, 21.30it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  58%|██▉  | 61/106 [00:02<00:02, 21.38it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  60%|███  | 64/106 [00:02<00:01, 21.01it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  63%|███▏ | 67/106 [00:02<00:01, 20.70it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  66%|███▎ | 70/106 [00:02<00:01, 20.74it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  69%|███▍ | 73/106 [00:03<00:01, 20.35it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  72%|███▌ | 76/106 [00:03<00:01, 20.33it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  75%|███▋ | 79/106 [00:03<00:01, 20.39it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  77%|███▊ | 82/106 [00:03<00:01, 20.62it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  80%|████ | 85/106 [00:03<00:01, 20.58it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  83%|████▏| 88/106 [00:03<00:00, 20.50it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  86%|████▎| 91/106 [00:03<00:00, 21.38it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  89%|████▍| 94/106 [00:04<00:00, 20.98it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  92%|████▌| 97/106 [00:04<00:00, 20.92it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  94%|███▊| 100/106 [00:04<00:00, 20.61it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset:  97%|███▉| 103/106 [00:04<00:00, 20.75it/s]\u001b[A\n",
      "epoch 004 | valid on 'valid' subset: 100%|████| 106/106 [00:04<00:00, 20.70it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 20:18:27 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 6.881 | ppl 117.89 | wps 46888.5 | wpb 2043.6 | bsz 4 | num_updates 12593 | best_loss 6.881\n",
      "2023-01-08 20:18:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 12593 updates\n",
      "2023-01-08 20:18:27 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint4.pt\n",
      "2023-01-08 20:18:30 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint4.pt\n",
      "2023-01-08 20:18:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint4.pt (epoch 4 @ 12593 updates, score 6.881) (writing took 4.794595872052014 seconds)\n",
      "2023-01-08 20:18:32 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
      "2023-01-08 20:18:32 | INFO | train | epoch 004 | loss 7.247 | ppl 151.92 | wps 16786.3 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 12593 | lr 1.9926e-05 | gnorm 0.853 | loss_scale 64 | train_wall 6100 | gb_free 15.4 | wall 24665\n",
      "2023-01-08 20:18:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 20:18:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 005:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 20:18:32 | INFO | fairseq.trainer | begin training epoch 5\n",
      "2023-01-08 20:18:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 005:   5%| | 150/3151 [04:52<1:37:32,  1.95s/it, loss=7.145, ppl=141.58, w2023-01-08 20:23:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 005:  40%|▍| 1248/3151 [40:34<1:02:16,  1.96s/it, loss=7.101, ppl=137.32, 2023-01-08 20:59:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 005:  73%|▋| 2301/3151 [1:15:01<27:48,  1.96s/it, loss=7.053, ppl=132.76, 2023-01-08 21:33:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 005: 100%|▉| 3150/3151 [1:42:42<00:01,  1.06s/it, loss=7.032, ppl=130.91, 2023-01-08 22:01:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 22:01:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 005 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   4%|▏     | 4/106 [00:00<00:02, 35.73it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:   8%|▍     | 8/106 [00:00<00:03, 25.03it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  10%|▌    | 11/106 [00:00<00:04, 23.26it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  13%|▋    | 14/106 [00:00<00:04, 21.93it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:04, 21.42it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:04, 21.13it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  22%|█    | 23/106 [00:01<00:04, 20.72it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  25%|█▏   | 26/106 [00:01<00:03, 20.66it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:01<00:03, 20.83it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  30%|█▌   | 32/106 [00:01<00:03, 20.72it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:01<00:03, 20.60it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  36%|█▊   | 38/106 [00:01<00:03, 20.92it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:01<00:02, 21.89it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  42%|██   | 44/106 [00:02<00:02, 21.23it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:02<00:02, 20.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  47%|██▎  | 50/106 [00:02<00:02, 21.00it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:02<00:02, 20.87it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  53%|██▋  | 56/106 [00:02<00:02, 20.81it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:02<00:02, 20.47it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  58%|██▉  | 62/106 [00:02<00:02, 20.27it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  61%|███  | 65/106 [00:03<00:02, 20.10it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  64%|███▏ | 68/106 [00:03<00:01, 20.02it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:03<00:01, 20.25it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  70%|███▍ | 74/106 [00:03<00:01, 21.50it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:03<00:01, 21.39it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  75%|███▊ | 80/106 [00:03<00:01, 20.90it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:03<00:01, 20.74it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  81%|████ | 86/106 [00:04<00:00, 20.58it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  84%|████▏| 89/106 [00:04<00:00, 20.72it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  87%|████▎| 92/106 [00:04<00:00, 20.67it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  90%|████▍| 95/106 [00:04<00:00, 20.57it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  92%|████▌| 98/106 [00:04<00:00, 20.65it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  95%|███▊| 101/106 [00:04<00:00, 20.53it/s]\u001b[A\n",
      "epoch 005 | valid on 'valid' subset:  98%|███▉| 104/106 [00:04<00:00, 20.59it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 22:01:20 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 6.735 | ppl 106.52 | wps 42734 | wpb 2043.6 | bsz 4 | num_updates 15741 | best_loss 6.735\n",
      "2023-01-08 22:01:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 15741 updates\n",
      "2023-01-08 22:01:20 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint5.pt\n",
      "2023-01-08 22:01:23 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint5.pt\n",
      "2023-01-08 22:01:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint5.pt (epoch 5 @ 15741 updates, score 6.735) (writing took 4.372768159024417 seconds)\n",
      "2023-01-08 22:01:25 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
      "2023-01-08 22:01:25 | INFO | train | epoch 005 | loss 7.076 | ppl 134.94 | wps 16707.2 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 15741 | lr 1.78225e-05 | gnorm 0.854 | loss_scale 64 | train_wall 6129 | gb_free 15.4 | wall 30837\n",
      "2023-01-08 22:01:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 22:01:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 006:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 22:01:25 | INFO | fairseq.trainer | begin training epoch 6\n",
      "2023-01-08 22:01:25 | INFO | fairseq_cli.train | Start iterating over samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 006:   6%| | 183/3151 [05:56<1:36:24,  1.95s/it, loss=6.989, ppl=127.06, w2023-01-08 22:07:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 006:  39%|▍| 1221/3151 [39:39<1:02:41,  1.95s/it, loss=6.969, ppl=125.32, 2023-01-08 22:41:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 006:  72%|▋| 2262/3151 [1:13:28<28:54,  1.95s/it, loss=6.924, ppl=121.42, 2023-01-08 23:14:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 006: 100%|▉| 3150/3151 [1:42:13<00:01,  1.07s/it, loss=6.9, ppl=119.43, wp2023-01-08 23:43:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
      "2023-01-08 23:43:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "\n",
      "epoch 006 | valid on 'valid' subset:   0%|              | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   5%|▎     | 5/106 [00:00<00:02, 42.45it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:   9%|▍    | 10/106 [00:00<00:02, 33.68it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  13%|▋    | 14/106 [00:00<00:03, 26.99it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  16%|▊    | 17/106 [00:00<00:03, 24.35it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  19%|▉    | 20/106 [00:00<00:03, 22.85it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  22%|█    | 23/106 [00:00<00:03, 21.91it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  25%|█▏   | 26/106 [00:01<00:03, 21.25it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  27%|█▎   | 29/106 [00:01<00:03, 21.09it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  30%|█▌   | 32/106 [00:01<00:03, 20.94it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  33%|█▋   | 35/106 [00:01<00:03, 21.03it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  36%|█▊   | 38/106 [00:01<00:03, 20.96it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  39%|█▉   | 41/106 [00:01<00:03, 20.77it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  42%|██   | 44/106 [00:01<00:02, 22.20it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  44%|██▏  | 47/106 [00:02<00:02, 21.27it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  47%|██▎  | 50/106 [00:02<00:02, 21.09it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  50%|██▌  | 53/106 [00:02<00:02, 20.95it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  53%|██▋  | 56/106 [00:02<00:02, 21.03it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  56%|██▊  | 59/106 [00:02<00:02, 20.87it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  58%|██▉  | 62/106 [00:02<00:02, 20.78it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  61%|███  | 65/106 [00:02<00:02, 20.34it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  64%|███▏ | 68/106 [00:03<00:01, 20.25it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  67%|███▎ | 71/106 [00:03<00:01, 20.31it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  70%|███▍ | 74/106 [00:03<00:01, 20.41it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  73%|███▋ | 77/106 [00:03<00:01, 20.81it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  75%|███▊ | 80/106 [00:03<00:01, 21.86it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  78%|███▉ | 83/106 [00:03<00:01, 21.69it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  81%|████ | 86/106 [00:03<00:00, 21.20it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  84%|████▏| 89/106 [00:04<00:00, 20.80it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  87%|████▎| 92/106 [00:04<00:00, 20.52it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  90%|████▍| 95/106 [00:04<00:00, 20.47it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  92%|████▌| 98/106 [00:04<00:00, 20.58it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  95%|███▊| 101/106 [00:04<00:00, 20.49it/s]\u001b[A\n",
      "epoch 006 | valid on 'valid' subset:  98%|███▉| 104/106 [00:04<00:00, 20.71it/s]\u001b[A\n",
      "                                                                                \u001b[A2023-01-08 23:43:44 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 6.621 | ppl 98.41 | wps 43802 | wpb 2043.6 | bsz 4 | num_updates 18889 | best_loss 6.621\n",
      "2023-01-08 23:43:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 18889 updates\n",
      "2023-01-08 23:43:44 | INFO | fairseq.trainer | Saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint6.pt\n",
      "2023-01-08 23:43:46 | INFO | fairseq.trainer | Finished saving checkpoint to /homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint6.pt\n",
      "2023-01-08 23:43:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/transformer_wikitext-103-mt2048-lr05/checkpoint6.pt (epoch 6 @ 18889 updates, score 6.621) (writing took 4.274791951989755 seconds)\n",
      "2023-01-08 23:43:48 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
      "2023-01-08 23:43:48 | INFO | train | epoch 006 | loss 6.947 | ppl 123.39 | wps 16787.2 | ups 0.51 | wpb 32759.4 | bsz 64 | num_updates 18889 | lr 1.62697e-05 | gnorm 0.861 | loss_scale 64 | train_wall 6100 | gb_free 15.4 | wall 36980\n",
      "2023-01-08 23:43:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
      "2023-01-08 23:43:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 3151\n",
      "epoch 007:   0%|                                       | 0/3151 [00:00<?, ?it/s]2023-01-08 23:43:48 | INFO | fairseq.trainer | begin training epoch 7\n",
      "2023-01-08 23:43:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
      "epoch 007:   5%| | 166/3151 [05:23<1:36:58,  1.95s/it, loss=6.875, ppl=117.35, w2023-01-08 23:49:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
      "epoch 007:  17%|▏| 550/3151 [17:47<50:24,  1.16s/it, loss=6.874, ppl=117.27, wps^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/bin/fairseq-train\", line 8, in <module>\n",
      "    sys.exit(cli_main())\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 574, in cli_main\n",
      "    distributed_utils.call_main(cfg, main)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
      "    main(cfg, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 205, in main\n",
      "    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq_cli/train.py\", line 331, in train\n",
      "    log_output = trainer.train_step(samples)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/contextlib.py\", line 75, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/trainer.py\", line 843, in train_step\n",
      "    loss, sample_size_i, logging_output = self.task.train_step(\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/tasks/fairseq_task.py\", line 531, in train_step\n",
      "    loss, sample_size, logging_output = criterion(model, sample)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/criterions/cross_entropy.py\", line 35, in forward\n",
      "    net_output = model(**sample[\"net_input\"])\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/fairseq_model.py\", line 506, in forward\n",
      "    return self.decoder(src_tokens, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/miniconda3/envs/galai/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 217, in forward\n",
      "    x, extra = self.extract_features(\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 239, in extract_features\n",
      "    return self.extract_features_scriptable(\n",
      "  File \"/homeB/zhuzhihao/recurrent/NLP_test/Introductory_tasks/fairseq/fairseq/models/transformer/transformer_decoder.py\", line 328, in extract_features_scriptable\n",
      "    if self.cross_self_attention or prev_output_tokens.eq(self.padding_idx).any():\n",
      "KeyboardInterrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=2 fairseq-train --task language_modeling \\\n",
    "  data-bin/wikitext-103 \\\n",
    "  --save-dir checkpoints/transformer_wikitext-103-mt2048-lr05 \\\n",
    "  --arch transformer_lm --share-decoder-input-output-embed \\\n",
    "  --dropout 0.1 \\\n",
    "  --optimizer adam --adam-betas '(0.9, 0.98)' --weight-decay 0.01 --clip-norm 0.0 \\\n",
    "  --lr 0.00005 --lr-scheduler inverse_sqrt --warmup-updates 2000 --warmup-init-lr 1e-07 \\\n",
    "  --tokens-per-sample 512 --sample-break-mode none \\\n",
    "  --max-tokens 2048 --update-freq 16 \\\n",
    "  --fp16 \\\n",
    "  --max-update 50000 \\\n",
    "  --wandb-project  NLP_test "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GALAI",
   "language": "python",
   "name": "galai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
